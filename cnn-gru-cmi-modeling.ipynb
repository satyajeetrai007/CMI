{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# preprocessing.py\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom scipy.spatial.transform import Rotation as R\nimport joblib\nimport gc\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==============================================================================\n#                            CONFIGURATION\n# ==============================================================================\n# --- File Paths ---\n# Adjust these paths based on your project structure\nDATA_DIR = '/kaggle/input/cmi-detect-behavior-with-sensor-data' # Example Kaggle path\nTRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\nTRAIN_DEMO_CSV = os.path.join(DATA_DIR, 'train_demographics.csv')\n\n# --- Preprocessing Parameters ---\nTOF_FILL_VALUE = 600 # Value to replace -1 in TOF\n\n# --- Output Paths ---\nOUTPUT_DIR = './data/preprocessed' # Save outputs here\nSCALER_PATH = os.path.join(OUTPUT_DIR, 'standard_scaler.joblib')\nLABEL_ENCODER_PATH = os.path.join(OUTPUT_DIR, 'label_encoder.joblib')\nIMU_COLS_PATH = os.path.join(OUTPUT_DIR, 'imu_feature_cols_3branch.pkl')\nTHM_COLS_PATH = os.path.join(OUTPUT_DIR, 'thm_feature_cols_3branch.pkl')\nTOF_COLS_PATH = os.path.join(OUTPUT_DIR, 'tof_feature_cols_3branch.pkl')\nALL_COLS_PATH = os.path.join(OUTPUT_DIR, 'all_feature_cols_3branch.pkl') # List of all scaled features\nPROCESSED_TRAIN_PATH = os.path.join(OUTPUT_DIR, 'train_processed.parquet') # Save processed data\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Output directory created/exists at: {OUTPUT_DIR}\")\n\n\n# ==============================================================================\n#                 HELPER FUNCTIONS (Preprocessing Steps)\n# ==============================================================================\n\n# --- Define Sensor Columns ---\n# Define these globally or pass them around as needed\nacc_cols = ['acc_x', 'acc_y', 'acc_z']\nrot_cols = ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n# Dynamically get thm/tof cols after loading\ntry:\n    _temp_df = pd.read_csv(TRAIN_CSV, nrows=0) # Read only header\n    thm_cols = sorted([col for col in _temp_df.columns if 'thm_' in col])\n    tof_cols = sorted([col for col in _temp_df.columns if 'tof_' in col])\n    del _temp_df\nexcept Exception as e:\n    print(f\"Warning: Could not dynamically determine thm/tof columns from header: {e}\")\n    # Fallback to static definition if dynamic fails\n    thm_cols = ['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5']\n    tof_cols = sorted([f'tof_{s}_v{i}' for s in range(1, 6) for i in range(64)])\n\n\nimu_cols = acc_cols + rot_cols\nnon_imu_sensor_cols = thm_cols + tof_cols\nall_initial_sensor_cols = imu_cols + non_imu_sensor_cols\n\nTARGET_GESTURES = [\n    'Above ear - pull hair', 'Cheek - pinch skin', 'Eyebrow - pull hair',\n    'Eyelash - pull hair', 'Forehead - pull hairline', 'Forehead - scratch',\n    'Neck - pinch skin', 'Neck - scratch'\n]\n\ndef preprocess_impute(df, tof_fill_value=600):\n    \"\"\"Handles imputation for sensor data.\"\"\"\n    print(f\"Imputing with TOF fill value: {tof_fill_value}...\")\n    # Ensure columns exist before modifying\n    present_tof_cols = [col for col in tof_cols if col in df.columns]\n    present_thm_cols = [col for col in thm_cols if col in df.columns]\n    present_all_initial_cols = [col for col in all_initial_sensor_cols if col in df.columns]\n\n    if present_tof_cols:\n        df[present_tof_cols] = df[present_tof_cols].replace(-1, tof_fill_value)\n    if present_thm_cols:\n        for col in present_thm_cols:\n             df[col] = df[col].apply(lambda x: np.nan if x < 20 else x)\n        if 'sequence_id' in df.columns:\n            # Use transform for interpolation within groups\n            # Add group_keys=False to avoid potential multi-index issues later\n            df[present_thm_cols] = df.groupby('sequence_id', group_keys=False)[present_thm_cols].transform(\n                lambda x: x.interpolate(method='linear', limit_direction='both', axis=0)\n            )\n        else: # Single sequence case\n            df[present_thm_cols] = df[present_thm_cols].interpolate(method='linear', limit_direction='both', axis=0)\n\n    group_cols_to_fill = list(set(present_all_initial_cols) - set(present_thm_cols))\n    if 'sequence_id' in df.columns and group_cols_to_fill:\n         df[group_cols_to_fill] = df.groupby('sequence_id', group_keys=False)[group_cols_to_fill].transform(\n             lambda x: x.ffill().bfill()\n         )\n    elif group_cols_to_fill: # Single sequence case\n         df[group_cols_to_fill] = df[group_cols_to_fill].ffill().bfill()\n\n    # Fill any remaining NaNs globally AFTER group operations\n    if present_all_initial_cols:\n        df[present_all_initial_cols] = df[present_all_initial_cols].fillna(0)\n    print(\"Imputation complete.\")\n    return df\n\ndef correct_handedness(df, demo_df):\n    \"\"\"Corrects sensor readings based on subject handedness.\"\"\"\n    print(\"Correcting handedness...\")\n    if 'handedness' not in df.columns:\n      if 'subject' in df.columns and 'subject' in demo_df.columns:\n          # Ensure subject columns are compatible type before merge\n          df['subject'] = df['subject'].astype(str)\n          demo_df['subject'] = demo_df['subject'].astype(str)\n          df = df.merge(demo_df[['subject', 'handedness']], on='subject', how='left')\n          # Handle potential NaNs in handedness after merge (e.g., test subjects not in demo)\n          df['handedness'] = df['handedness'].fillna('Right')\n      else:\n          df['handedness'] = 'Right' # Fallback if subject info missing\n\n    left_handed_mask = df['handedness'] == 'Left'\n    if left_handed_mask.any():\n        if 'acc_x' in df.columns: df.loc[left_handed_mask, 'acc_x'] *= -1\n        if 'rot_y' in df.columns: df.loc[left_handed_mask, 'rot_y'] *= -1\n        if 'rot_z' in df.columns: df.loc[left_handed_mask, 'rot_z'] *= -1\n        # Add TOF/THM mirroring here if implementing\n    print(\"Handedness correction complete.\")\n    return df\n\ndef correct_upside_down(df):\n    \"\"\"Corrects sensor readings for known upside-down subjects.\"\"\"\n    print(\"Correcting upside-down subjects...\")\n    upside_down_subjects = ['SUBJ_019262', 'SUBJ_045235']\n    if 'subject' in df.columns:\n        df['subject'] = df['subject'].astype(str) # Ensure subject is string\n        ud_mask = df['subject'].isin(upside_down_subjects)\n        if ud_mask.any():\n            if all(c in df.columns for c in ['acc_x', 'acc_y']): df.loc[ud_mask, ['acc_x', 'acc_y']] *= -1\n            if all(c in df.columns for c in ['rot_x', 'rot_y']): df.loc[ud_mask, ['rot_x', 'rot_y']] *= -1\n            # Add TOF/THM mirroring here if implementing\n    print(\"Upside-down correction complete.\")\n    return df\n\ndef add_linear_acceleration(df):\n    \"\"\"Calculates linear acceleration by removing gravity.\"\"\"\n    print(\"Calculating linear acceleration...\")\n    if not all(col in df.columns for col in rot_cols + acc_cols):\n        print(\"Warning: Acc/Rot columns missing. Assigning raw acceleration to linear.\")\n        # Ensure target columns exist before assignment\n        present_acc_cols = [c for c in acc_cols if c in df.columns]\n        lin_acc_map = {'acc_x': 'lin_acc_x', 'acc_y': 'lin_acc_y', 'acc_z': 'lin_acc_z'}\n        for col in acc_cols:\n             df[lin_acc_map[col]] = df[col] if col in df.columns else 0.0\n        return df\n\n    # Ensure float type before processing\n    for col in rot_cols + acc_cols:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n\n    quats = df[rot_cols].values\n    accels = df[acc_cols].values\n    linear_accel = np.zeros_like(accels)\n    gravity_world = np.array([0, 0, 1.0]) # Z-up, magnitude 1\n\n    quats_numeric = np.nan_to_num(quats) # Replace NaN with 0 for checks\n    # Check for non-zero quaternions and approximate unit norm\n    valid_quat_mask = ~np.all(quats_numeric == 0, axis=1) & (np.abs(np.linalg.norm(quats_numeric, axis=1) - 1.0) < 1e-2) # Adjusted tolerance\n    valid_quats_data = quats_numeric[valid_quat_mask]\n\n    if len(valid_quats_data) > 0:\n        try:\n            # Rigorous normalization\n            norms = np.linalg.norm(valid_quats_data, axis=1, keepdims=True)\n            # Prevent division by zero or very small norms\n            norms[norms < 1e-6] = 1.0\n            valid_quats_normalized = valid_quats_data / norms\n            # Clip to ensure values are within valid range for R.from_quat\n            valid_quats_normalized = np.clip(valid_quats_normalized, -1.0, 1.0)\n\n            r = R.from_quat(valid_quats_normalized)\n            r_inv = r.inv()\n            gravity_sensor_frame = r_inv.apply(gravity_world)\n            linear_accel[valid_quat_mask] = accels[valid_quat_mask] - gravity_sensor_frame\n        except Exception as e:\n            print(f\"Warning: Scipy Rotation error during gravity removal: {e}. Using raw accel for some rows.\")\n            linear_accel[valid_quat_mask] = accels[valid_quat_mask] # Fallback\n\n    invalid_quat_mask = ~valid_quat_mask\n    linear_accel[invalid_quat_mask] = accels[invalid_quat_mask] # Use raw accel if quat invalid\n\n    df['lin_acc_x'], df['lin_acc_y'], df['lin_acc_z'] = linear_accel[:, 0], linear_accel[:, 1], linear_accel[:, 2]\n    print(\"Linear acceleration added.\")\n    return df\n\ndef add_basic_features(df):\n    \"\"\"Adds magnitude features.\"\"\"\n    print(\"Adding basic features (magnitudes)...\")\n     # Ensure float type\n    acc_mag_cols = ['acc_x', 'acc_y', 'acc_z']\n    lin_acc_mag_cols = ['lin_acc_x', 'lin_acc_y', 'lin_acc_z']\n    rot_mag_cols = ['rot_x', 'rot_y', 'rot_z']\n    for col in acc_mag_cols + lin_acc_mag_cols + rot_mag_cols:\n         if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n\n    if all(col in df.columns for col in acc_mag_cols):\n        df['acc_mag'] = np.linalg.norm(df[acc_mag_cols].values, axis=1)\n    else: df['acc_mag'] = 0.0\n\n    if all(col in df.columns for col in lin_acc_mag_cols):\n        df['lin_acc_mag'] = np.linalg.norm(df[lin_acc_mag_cols].values, axis=1)\n    else: df['lin_acc_mag'] = 0.0\n\n    if all(col in df.columns for col in rot_mag_cols):\n        df['rot_mag'] = np.linalg.norm(df[rot_mag_cols].values, axis=1)\n    else: df['rot_mag'] = 0.0\n    print(\"Basic features added.\")\n    return df\n\ndef full_preprocess_pipeline(df, demo_df, scaler=None, fit_scaler=False):\n    \"\"\"Applies the full preprocessing pipeline using CPU libraries.\"\"\"\n    print(\"-\" * 30)\n    print(\"Starting Full Preprocessing Pipeline...\")\n    # Ensure IDs are numeric/string\n    if 'sequence_id' in df.columns and df['sequence_id'].dtype == 'object':\n         df['sequence_id'] = df['sequence_id'].str.extract('(\\d+)').astype(int)\n    if 'subject' in df.columns: df['subject'] = df['subject'].astype(str)\n    if 'subject' in demo_df.columns: demo_df['subject'] = demo_df['subject'].astype(str)\n\n    # --- Preprocessing Steps ---\n    df_processed = preprocess_impute(df.copy(), tof_fill_value=TOF_FILL_VALUE)\n    df_processed = correct_handedness(df_processed, demo_df)\n    df_processed = correct_upside_down(df_processed)\n    df_processed = add_linear_acceleration(df_processed)\n    df_processed = add_basic_features(df_processed)\n\n    # --- Scaling ---\n    engineered_imu_cols = ['lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'acc_mag', 'lin_acc_mag', 'rot_mag']\n    potential_cols_to_scale = list(set(imu_cols + non_imu_sensor_cols + engineered_imu_cols))\n    # Ensure columns exist and sort for consistent order\n    cols_to_scale = sorted([col for col in potential_cols_to_scale if col in df_processed.columns])\n\n    if not cols_to_scale:\n         print(\"Warning: No columns identified for scaling.\")\n         # Add feature_names_in_ attribute if scaler exists but no columns to scale\n         if scaler is not None and not hasattr(scaler, 'feature_names_in_'):\n             scaler.feature_names_in_ = []\n         return df_processed, scaler, cols_to_scale # Return early if no cols\n\n    if fit_scaler:\n        print(f\"Fitting StandardScaler on {len(cols_to_scale)} features.\")\n        scaler = StandardScaler()\n        # Ensure data is numeric before fitting\n        df_processed[cols_to_scale] = df_processed[cols_to_scale].apply(pd.to_numeric, errors='coerce').fillna(0)\n        df_processed[cols_to_scale] = scaler.fit_transform(df_processed[cols_to_scale])\n        # Store feature names IN THE ORDER THEY WERE FITTED\n        scaler.feature_names_in_ = cols_to_scale\n        print(\"Scaler fitted.\")\n    elif scaler is not None:\n        print(f\"Transforming features using loaded scaler...\")\n        # Ensure scaler has feature names\n        if not hasattr(scaler, 'feature_names_in_') or scaler.feature_names_in_ is None:\n             raise ValueError(\"Loaded scaler is missing 'feature_names_in_'. Cannot proceed.\")\n\n        # Ensure columns match scaler's expectations\n        cols_available_ordered = [col for col in scaler.feature_names_in_ if col in df_processed.columns]\n        # Create a DataFrame with the exact columns and order the scaler expects\n        df_ordered = pd.DataFrame(0.0, index=df_processed.index, columns=scaler.feature_names_in_)\n        # Fill with available data, respecting scaler's column order\n        if cols_available_ordered: # Check if there are any columns to fill\n            df_ordered[cols_available_ordered] = df_processed[cols_available_ordered]\n        else:\n             print(\"Warning: No columns from scaler found in the dataframe to transform.\")\n\n        if len(cols_available_ordered) < len(scaler.feature_names_in_):\n            missing_cols = list(set(scaler.feature_names_in_) - set(cols_available_ordered))\n            print(f\"Warning: Filled {len(missing_cols)} missing columns with zeros before scaling: {missing_cols}\")\n        # Ensure data is numeric before transforming\n        df_ordered[scaler.feature_names_in_] = df_ordered[scaler.feature_names_in_].apply(pd.to_numeric, errors='coerce').fillna(0)\n        # Transform using the reordered/filled dataframe\n        df_processed[scaler.feature_names_in_] = scaler.transform(df_ordered[scaler.feature_names_in_])\n        print(\"Transformation complete.\")\n    else:\n        raise ValueError(\"Scaler must be provided if fit_scaler is False\")\n\n    # Drop handedness column after scaling (if it wasn't used as feature)\n    if 'handedness' in df_processed.columns and 'handedness' not in cols_to_scale:\n        df_processed = df_processed.drop(columns=['handedness'])\n\n    print(\"Full Preprocessing Pipeline Finished.\")\n    print(\"-\" * 30)\n    return df_processed, scaler, cols_to_scale\n\n# ==============================================================================\n#                      MAIN PREPROCESSING EXECUTION\n# ==============================================================================\nprint(\"Running Main Preprocessing Script...\")\n\n# --- Load Data ---\nprint(\"Loading data...\")\ntry:\n    train_df_main = pd.read_csv(TRAIN_CSV)\n    train_demo_df_main = pd.read_csv(TRAIN_DEMO_CSV)\n    print(f\"Loaded Train shape: {train_df_main.shape}\")\nexcept FileNotFoundError:\n    print(f\"Error: Training files not found at {DATA_DIR}. Exiting.\")\n    exit()\nexcept Exception as e:\n    print(f\"Error loading data: {e}. Exiting.\")\n    exit()\n\n# --- Apply Full Preprocessing to Training Data ---\ntrain_processed_df_main, scaler_main, feature_columns_main = full_preprocess_pipeline(\n    train_df_main, train_demo_df_main, scaler=None, fit_scaler=True\n)\nprint(f\"Processed Train shape: {train_processed_df_main.shape}\")\nprint(f\"Number of features scaled: {len(feature_columns_main)}\")\n\n# --- Define Feature Lists for Branches (Example) ---\nimu_eng_cols_main = ['lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'acc_mag', 'lin_acc_mag', 'rot_mag']\nmodel_imu_feature_cols_main = sorted([col for col in feature_columns_main if col in imu_cols + imu_eng_cols_main])\nmodel_thm_feature_cols_main = sorted([col for col in feature_columns_main if col in thm_cols])\nmodel_tof_feature_cols_main = sorted([col for col in feature_columns_main if col in tof_cols])\n\nN_FEATURES_IMU_main = len(model_imu_feature_cols_main)\nN_FEATURES_THM_main = len(model_thm_feature_cols_main)\nN_FEATURES_TOF_main = len(model_tof_feature_cols_main)\nprint(f\"Branch Features: IMU={N_FEATURES_IMU_main}, THM={N_FEATURES_THM_main}, TOF={N_FEATURES_TOF_main}\")\n\n\n# --- Save Objects ---\nprint(\"\\nSaving preprocessor objects...\")\njoblib.dump(scaler_main, SCALER_PATH)\njoblib.dump(model_imu_feature_cols_main, IMU_COLS_PATH)\njoblib.dump(model_thm_feature_cols_main, THM_COLS_PATH)\njoblib.dump(model_tof_feature_cols_main, TOF_COLS_PATH)\njoblib.dump(feature_columns_main, ALL_COLS_PATH) # Save list of all scaled features\nprint(f\"Objects saved to: {OUTPUT_DIR}\")\n\n# --- Label Encode and Save Encoder ---\nprint(\"Encoding target and saving encoder...\")\nlabel_encoder_main = LabelEncoder()\nif 'gesture' in train_processed_df_main.columns:\n    train_processed_df_main['gesture_encoded'] = label_encoder_main.fit_transform(train_processed_df_main['gesture'])\n    joblib.dump(label_encoder_main, LABEL_ENCODER_PATH)\n    print(f\"Target Classes ({len(label_encoder_main.classes_)}): {label_encoder_main.classes_}\")\n    print(f\"Label encoder saved to: {LABEL_ENCODER_PATH}\")\nelse:\n    print(\"Warning: 'gesture' column not found in processed data. Cannot save label encoder.\")\n\n# --- Save Processed Data (Optional) ---\nprint(f\"\\nSaving processed training data to {PROCESSED_TRAIN_PATH}...\")\ntry:\n    # Ensure directory exists before saving\n    os.makedirs(os.path.dirname(PROCESSED_TRAIN_PATH), exist_ok=True)\n    # Ensure correct types before saving to parquet\n    df_to_save = train_processed_df_main.copy()\n    for col in df_to_save.select_dtypes(include='object').columns:\n        # Check if column still exists before converting\n        if col in df_to_save.columns:\n            df_to_save[col] = df_to_save[col].astype(str) # Convert object cols to string\n\n    df_to_save.to_parquet(PROCESSED_TRAIN_PATH, index=False)\n    print(\"Processed data saved.\")\nexcept Exception as e:\n    print(f\"Error saving processed data: {e}\")\n\nprint(\"\\nPreprocessing script finished.\")\n\n# Clean up large dataframes from memory\ndel train_df_main, train_demo_df_main, train_processed_df_main\ngc.collect()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T15:13:00.864192Z","iopub.execute_input":"2025-10-23T15:13:00.864471Z","iopub.status.idle":"2025-10-23T15:17:00.908567Z","shell.execute_reply.started":"2025-10-23T15:13:00.864448Z","shell.execute_reply":"2025-10-23T15:17:00.907801Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Output directory created/exists at: ./data/preprocessed\nRunning Main Preprocessing Script...\nLoading data...\nLoaded Train shape: (574945, 341)\n------------------------------\nStarting Full Preprocessing Pipeline...\nImputing with TOF fill value: 600...\nImputation complete.\nCorrecting handedness...\nHandedness correction complete.\nCorrecting upside-down subjects...\nUpside-down correction complete.\nCalculating linear acceleration...\nLinear acceleration added.\nAdding basic features (magnitudes)...\nBasic features added.\nFitting StandardScaler on 338 features.\nScaler fitted.\nFull Preprocessing Pipeline Finished.\n------------------------------\nProcessed Train shape: (574945, 347)\nNumber of features scaled: 338\nBranch Features: IMU=13, THM=5, TOF=320\n\nSaving preprocessor objects...\nObjects saved to: ./data/preprocessed\nEncoding target and saving encoder...\nTarget Classes (18): ['Above ear - pull hair' 'Cheek - pinch skin' 'Drink from bottle/cup'\n 'Eyebrow - pull hair' 'Eyelash - pull hair'\n 'Feel around in tray and pull out an object' 'Forehead - pull hairline'\n 'Forehead - scratch' 'Glasses on/off' 'Neck - pinch skin'\n 'Neck - scratch' 'Pinch knee/leg skin' 'Pull air toward your face'\n 'Scratch knee/leg skin' 'Text on phone' 'Wave hello' 'Write name in air'\n 'Write name on leg']\nLabel encoder saved to: ./data/preprocessed/label_encoder.joblib\n\nSaving processed training data to ./data/preprocessed/train_processed.parquet...\nProcessed data saved.\n\nPreprocessing script finished.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T15:17:00.911865Z","iopub.execute_input":"2025-10-23T15:17:00.912121Z","iopub.status.idle":"2025-10-23T15:17:00.917097Z","shell.execute_reply.started":"2025-10-23T15:17:00.912097Z","shell.execute_reply":"2025-10-23T15:17:00.916403Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# data_preparation.py\n\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport os\nfrom tqdm import tqdm # For progress bar\nimport gc\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==============================================================================\n#                            CONFIGURATION\n# ==============================================================================\n# --- Input Paths ---\n# Should match the OUTPUT_DIR and filenames from the *preprocessing script*\nPREPROCESSED_DIR = './data/preprocessed' # Directory where preprocessed files ARE SAVED\nPROCESSED_TRAIN_PATH = os.path.join(PREPROCESSED_DIR, 'train_processed.parquet')\nALL_COLS_PATH = os.path.join(PREPROCESSED_DIR, 'all_feature_cols_3branch.pkl') # List of all scaled columns\nLABEL_ENCODER_PATH = os.path.join(PREPROCESSED_DIR, 'label_encoder.joblib') # Load encoder to ensure y-values are valid\n\n# --- Model Input Parameters ---\nMAX_LENGTH = 192 # Sequence length for padding/truncating\n\n# --- Output Paths ---\n# Where to SAVE the final NumPy arrays for the model\nOUTPUT_NP_DIR = './data/model_input'\nX_PATH = os.path.join(OUTPUT_NP_DIR, 'X_train.npy')\nY_PATH = os.path.join(OUTPUT_NP_DIR, 'y_train.npy')\nGROUPS_PATH = os.path.join(OUTPUT_NP_DIR, 'groups_train.npy')\nSEQ_IDS_PATH = os.path.join(OUTPUT_NP_DIR, 'train_seq_ids.npy') # Path to save sequence IDs\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_NP_DIR, exist_ok=True)\nprint(f\"NumPy output directory created/exists at: {OUTPUT_NP_DIR}\")\n\n\n# ==============================================================================\n#                      LOAD PREPROCESSED DATA & OBJECTS\n# ==============================================================================\nprint(\"Loading preprocessed data and feature lists...\")\ntry:\n    train_processed_df = pd.read_parquet(PROCESSED_TRAIN_PATH)\n    # Load the list of all columns that were included after preprocessing\n    final_column_names = joblib.load(ALL_COLS_PATH)\n    # Load Label Encoder to validate labels later if needed\n    label_encoder = joblib.load(LABEL_ENCODER_PATH)\n    N_CLASSES = len(label_encoder.classes_) # Get number of classes\n\n    # --- Derive feature_cols from the loaded list ---\n    # Define columns that are NOT features\n    non_feature_cols = [\n        'sequence_id', 'subject', 'gesture', 'row_id', 'sequence_counter',\n        'orientation', 'behavior', 'phase', 'sequence_type', 'handedness', # If kept during preprocessing\n        'gesture_encoded' # The target label\n    ]\n    # Filter final_column_names to get only feature columns, ensure they exist\n    feature_cols = sorted([\n        col for col in final_column_names\n        if col not in non_feature_cols and col in train_processed_df.columns\n    ])\n\n    print(f\"Loaded processed data with shape: {train_processed_df.shape}\")\n    print(f\"Identified {len(feature_cols)} feature columns to use.\")\n    if len(feature_cols) == 0:\n        raise ValueError(\"No feature columns identified after filtering. Check ALL_COLS_PATH and non_feature_cols.\")\n    # print(\"Feature columns:\", feature_cols) # Uncomment to verify features\n\n    # Verify essential columns exist\n    if 'sequence_id' not in train_processed_df.columns: raise ValueError(\"Missing 'sequence_id'\")\n    if 'gesture_encoded' not in train_processed_df.columns: raise ValueError(\"Missing 'gesture_encoded'\")\n    if 'subject' not in train_processed_df.columns: raise ValueError(\"Missing 'subject'\")\n\nexcept FileNotFoundError as e:\n    print(f\"Error: Required file not found. Ensure preprocessing script ran successfully and saved outputs to '{PREPROCESSED_DIR}'. Missing: {e.filename}\")\n    exit()\nexcept Exception as e:\n    print(f\"An error occurred during loading: {e}\")\n    exit()\n\n# ==============================================================================\n#                      PREPARE MODEL INPUT ARRAYS\n# ==============================================================================\nprint(\"\\nPreparing model input arrays (Grouping, Padding/Truncating)...\")\n\nall_sequences = []\nall_labels = []\nall_groups = []\nall_sequence_ids = [] # List to store sequence IDs\nnum_features = len(feature_cols) # Use the derived feature_cols list\n\n# Group by sequence ID\ngrouped_data = train_processed_df.groupby('sequence_id')\ntotal_sequences = len(grouped_data)\nprint(f\"Processing {total_sequences} sequences...\")\nskipped_sequences_prep = 0\n\n# Iterate through each sequence group with a progress bar\nfor name, group in tqdm(grouped_data, total=total_sequences, desc=\"Processing Sequences\"):\n    # Extract features, label, and group\n    try:\n        # Check for NaNs *before* accessing values\n        if group[feature_cols].isnull().values.any():\n             # print(f\"Warning: Sequence {name} contains NaN values. Skipping.\") # Can be verbose\n             skipped_sequences_prep += 1\n             continue\n\n        sequence_features = group[feature_cols].values.astype(np.float32) # Ensure float32\n        label = group['gesture_encoded'].iloc[0] # Get the single label for the sequence\n        subject = group['subject'].iloc[0]       # Get the subject ID\n\n        current_length = sequence_features.shape[0]\n\n        if current_length == 0:\n            # print(f\"Warning: Sequence {name} is empty after filtering NaNs. Skipping.\") # Can be verbose\n            skipped_sequences_prep += 1\n            continue # Skip empty sequences\n\n        # Validate label is within expected range\n        if not (0 <= label < N_CLASSES):\n            # print(f\"Warning: Sequence {name} has invalid label {label}. Skipping.\")\n            skipped_sequences_prep += 1\n            continue\n\n        padded_sequence = np.zeros((MAX_LENGTH, num_features), dtype=np.float32) # Initialize with zeros\n\n        # Pad or Truncate (using pre-padding)\n        if current_length >= MAX_LENGTH:\n            # Truncate: Take the last MAX_LENGTH steps\n            padded_sequence = sequence_features[-MAX_LENGTH:]\n        else:\n            # Pad: Add zeros at the beginning (pre-padding)\n            pad_width = MAX_LENGTH - current_length\n            padded_sequence[pad_width:] = sequence_features\n\n        # Append results to lists\n        all_sequences.append(padded_sequence)\n        all_labels.append(label)\n        all_groups.append(subject)\n        all_sequence_ids.append(name) # STORE SEQUENCE ID\n\n    except Exception as e:\n        print(f\"Error processing sequence {name}: {e}. Skipping.\")\n        skipped_sequences_prep += 1\n        continue\n\nprint(f\"Skipped {skipped_sequences_prep} sequences during preparation due to issues.\")\n\n# Convert lists to NumPy arrays\nif not all_sequences:\n    print(\"FATAL ERROR: No valid sequences were processed. Cannot create NumPy arrays.\")\n    exit()\n\nX = np.array(all_sequences)\ny = np.array(all_labels)\ngroups = np.array(all_groups)\ntrain_seq_ids = np.array(all_sequence_ids) # CONVERT SEQ IDS TO NUMPY ARRAY\n\n\nprint(\"\\nModel input preparation complete.\")\nprint(f\"Shape of X: {X.shape}\") # Should be (num_sequences, MAX_LENGTH, num_features)\nprint(f\"Shape of y: {y.shape}\")   # Should be (num_sequences,)\nprint(f\"Shape of groups: {groups.shape}\") # Should be (num_sequences,)\nprint(f\"Shape of train_seq_ids: {train_seq_ids.shape}\")\n\n# ==============================================================================\n#                      SAVE MODEL INPUT ARRAYS\n# ==============================================================================\nprint(f\"\\nSaving prepared NumPy arrays to {OUTPUT_NP_DIR}...\")\ntry:\n    np.save(X_PATH, X)\n    np.save(Y_PATH, y)\n    np.save(GROUPS_PATH, groups)\n    np.save(SEQ_IDS_PATH, train_seq_ids) # SAVE SEQUENCE IDS\n    print(\"Arrays saved successfully.\")\nexcept Exception as e:\n    print(f\"Error saving NumPy arrays: {e}\")\n\nprint(\"\\nData Preparation script finished. You now have X, y, groups, and train_seq_ids ready for model training.\")\n\n# Clean up memory\ndel train_processed_df, X, y, groups, train_seq_ids, all_sequences, all_labels, all_groups, all_sequence_ids\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T15:17:00.918501Z","iopub.execute_input":"2025-10-23T15:17:00.918702Z","iopub.status.idle":"2025-10-23T15:17:16.051712Z","shell.execute_reply.started":"2025-10-23T15:17:00.918686Z","shell.execute_reply":"2025-10-23T15:17:16.050739Z"}},"outputs":[{"name":"stdout","text":"NumPy output directory created/exists at: ./data/model_input\nLoading preprocessed data and feature lists...\nLoaded processed data with shape: (574945, 348)\nIdentified 338 feature columns to use.\n\nPreparing model input arrays (Grouping, Padding/Truncating)...\nProcessing 8151 sequences...\n","output_type":"stream"},{"name":"stderr","text":"Processing Sequences: 100%|██████████| 8151/8151 [00:11<00:00, 737.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Skipped 0 sequences during preparation due to issues.\n\nModel input preparation complete.\nShape of X: (8151, 192, 338)\nShape of y: (8151,)\nShape of groups: (8151,)\nShape of train_seq_ids: (8151,)\n\nSaving prepared NumPy arrays to ./data/model_input...\nArrays saved successfully.\n\nData Preparation script finished. You now have X, y, groups, and train_seq_ids ready for model training.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.optimize import linear_sum_assignment\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport joblib\nimport gc\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nprint(\"TensorFlow Version:\", tf.__version__)\n\n# ==============================================================================\n#                            CONFIGURATION\n# ==============================================================================\nN_SPLITS = 2\nBATCH_SIZE = 128 # Adjust based on memory\nEPOCHS = 50 # Adjust based on validation performance\nMAX_LENGTH = 192 # Must match the MAX_LENGTH used in data preparation\nSEED = 42\nSENSOR_DROPOUT_RATE = 0.5 # Probability of dropping non-IMU sensors during training\n\n# Dropout Rates for Model\nCNN_SPATIAL_DROPOUT_RATE = 0.25\nLSTM_DROPOUT_RATE = 0.4 # Using GRU, applied to GRU\nLSTM_RECURRENT_DROPOUT_RATE = 0.4 # Applied to GRU\nDENSE_DROPOUT_RATE = 0.5\n\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# Paths for LOADING objects and prepared arrays\nPREPROCESSED_DIR = './data/preprocessed'\nLABEL_ENCODER_PATH = os.path.join(PREPROCESSED_DIR, 'label_encoder.joblib')\nALL_COLS_PATH = os.path.join(PREPROCESSED_DIR, 'all_feature_cols_3branch.pkl') # List of all columns in X\nIMU_COLS_PATH = os.path.join(PREPROCESSED_DIR, 'imu_feature_cols_3branch.pkl')\nTHM_COLS_PATH = os.path.join(PREPROCESSED_DIR, 'thm_feature_cols_3branch.pkl')\nTOF_COLS_PATH = os.path.join(PREPROCESSED_DIR, 'tof_feature_cols_3branch.pkl')\n\nNUMPY_INPUT_DIR = './data/model_input' # Where X, y, groups were saved\nX_PATH = os.path.join(NUMPY_INPUT_DIR, 'X_train.npy')\nY_PATH = os.path.join(NUMPY_INPUT_DIR, 'y_train.npy')\nGROUPS_PATH = os.path.join(NUMPY_INPUT_DIR, 'groups_train.npy')\nSEQ_IDS_PATH = os.path.join(NUMPY_INPUT_DIR, 'train_seq_ids.npy') # Load if needed for PP\n\n# Paths for saving NEW models/results\nOUTPUT_DIR = './data/3branch_dropout_output' # New directory for this model run\nMODEL_TMPL = os.path.join(OUTPUT_DIR, \"model_3branch_dropout_fold_{fold}.keras\")\nOOF_CSV_PATH = os.path.join(OUTPUT_DIR, \"oof_predictions_3branch_dropout.csv\")\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# --- Target Gestures ---\nTARGET_GESTURES = [\n    'Above ear - pull hair', 'Cheek - pinch skin', 'Eyebrow - pull hair',\n    'Eyelash - pull hair', 'Forehead - pull hairline', 'Forehead - scratch',\n    'Neck - pinch skin', 'Neck - scratch'\n]\n# TOF Reshaping dimensions\nTOF_SENSORS = 5\nTOF_GRID_DIM = 8\n\n# ==============================================================================\n#               LOAD PREPARED DATA & PREPROCESSING OBJECTS\n# ==============================================================================\nprint(\"Loading pre-trained objects and prepared NumPy arrays...\")\ntry:\n    label_encoder = joblib.load(LABEL_ENCODER_PATH)\n    all_feature_cols = joblib.load(ALL_COLS_PATH) # List of columns in X, in order\n    model_imu_feature_cols = joblib.load(IMU_COLS_PATH)\n    model_thm_feature_cols = joblib.load(THM_COLS_PATH)\n    model_tof_feature_cols = joblib.load(TOF_COLS_PATH)\n\n    N_CLASSES = len(label_encoder.classes_)\n    N_FEATURES_ALL = len(all_feature_cols) # Total features in X\n\n    # Load NumPy arrays\n    X = np.load(X_PATH)\n    y = np.load(Y_PATH)\n    groups = np.load(GROUPS_PATH) # Subject IDs\n    train_seq_ids = np.load(SEQ_IDS_PATH) # Load sequence IDs if saved and needed\n\n    # --- Find indices for splitting X into branches ---\n    # Create mapping from column name to index in the full X array\n    col_to_idx = {col: idx for idx, col in enumerate(all_feature_cols)}\n    imu_indices = sorted([col_to_idx[col] for col in model_imu_feature_cols if col in col_to_idx])\n    thm_indices = sorted([col_to_idx[col] for col in model_thm_feature_cols if col in col_to_idx])\n    tof_indices = sorted([col_to_idx[col] for col in model_tof_feature_cols if col in col_to_idx])\n\n    N_FEATURES_IMU = len(imu_indices)\n    N_FEATURES_THM = len(thm_indices)\n    N_FEATURES_TOF = len(tof_indices) # Should be 320\n\n    # Calculate scaled missing value indicator (Scaled Zero) - requires scaler\n    # Since scaler wasn't loaded here, assume 0.0 is the padding value\n    # If using StandardScaler and filled NaNs with 0 before scaling, this is likely correct\n    MISSING_NON_IMU_SCALED_VALUE = 0.0\n    print(f\"Using assumed scaled missing non-IMU indicator value: {MISSING_NON_IMU_SCALED_VALUE:.4f}\")\n\n    print(\"Loaded NumPy arrays and found feature indices:\")\n    print(f\"X shape: {X.shape} (Expected features: {N_FEATURES_ALL})\")\n    print(f\"y shape: {y.shape}, groups shape: {groups.shape}\")\n    print(f\"IMU features: {N_FEATURES_IMU}, THM features: {N_FEATURES_THM}, TOF features: {N_FEATURES_TOF}\")\n    print(f\"Number of classes: {N_CLASSES}\")\n\n    # Basic shape validation\n    if X.shape[0] != len(y) or X.shape[0] != len(groups):\n        raise ValueError(\"Loaded X, y, and groups have inconsistent lengths.\")\n    if X.shape[2] != N_FEATURES_ALL:\n        raise ValueError(f\"Loaded X feature dimension ({X.shape[2]}) doesn't match expected ({N_FEATURES_ALL}).\")\n    if N_FEATURES_TOF != TOF_SENSORS * TOF_GRID_DIM * TOF_GRID_DIM:\n         print(f\"Warning: TOF feature count ({N_FEATURES_TOF}) mismatch expected ({TOF_SENSORS*TOF_GRID_DIM**2}).\")\n\n\nexcept FileNotFoundError as e:\n    print(f\"FATAL ERROR: Required file not found. Ensure previous script saved arrays/objects. Missing: {e.filename}\")\n    exit()\nexcept Exception as e:\n    print(f\"FATAL ERROR loading data or objects: {e}\")\n    exit()\n\n\n# ==============================================================================\n#      SPLIT X INTO BRANCHES & PREPARE FOR TF.DATA\n# ==============================================================================\nprint(\"\\nSplitting X into IMU, THM, TOF branches...\")\nX_imu = X[:, :, imu_indices]\nX_thm = X[:, :, thm_indices]\nX_tof_flat = X[:, :, tof_indices] # Shape (n_seq, max_len, 320)\n\n# Reshape TOF now for the tf.data pipeline input\nX_tof = X_tof_flat.reshape((-1, MAX_LENGTH, TOF_GRID_DIM, TOF_GRID_DIM, TOF_SENSORS))\nprint(f\"Split array shapes: X_imu={X_imu.shape}, X_thm={X_thm.shape}, X_tof={X_tof.shape}\")\n\ndel X, X_tof_flat # Free memory of the large combined array\ngc.collect()\n\n# ==============================================================================\n#                 tf.data Pipeline with Sensor Dropout\n# ==============================================================================\n\ndef sensor_dropout_tf_fn(features_dict, label):\n    \"\"\"Applies sensor dropout augmentation to THM and TOF tensors.\"\"\"\n    imu_feat = features_dict['imu_input']\n    thm_feat = features_dict['thm_input']\n    tof_feat = features_dict['tof_input']\n\n    if tf.random.uniform(()) < SENSOR_DROPOUT_RATE:\n        # Replace THM and TOF features with the scaled missing value (zeros)\n        thm_feat = tf.zeros_like(thm_feat) # Use zeros matching padding value\n        tof_feat = tf.zeros_like(tof_feat) # Use zeros matching padding value\n\n    # Return updated dictionary\n    return {'imu_input': imu_feat, 'thm_input': thm_feat, 'tof_input': tof_feat}, label\n\n\ndef create_tf_dataset_from_np(X_imu_np, X_thm_np, X_tof_np, y_np, batch_size, shuffle=False, augment=False):\n    \"\"\"Creates a tf.data.Dataset directly from NumPy arrays for 3 branches.\"\"\"\n    # Create input dictionary from NumPy slices\n    input_dict = {'imu_input': X_imu_np, 'thm_input': X_thm_np, 'tof_input': X_tof_np}\n\n    # Create dataset from slices\n    dataset = tf.data.Dataset.from_tensor_slices((input_dict, y_np))\n\n    if shuffle:\n        dataset = dataset.shuffle(len(y_np), seed=SEED, reshuffle_each_iteration=True)\n\n    if augment:\n        dataset = dataset.map(sensor_dropout_tf_fn, num_parallel_calls=tf.data.AUTOTUNE)\n\n    # Batch the dataset - Padding is already done, so just batch\n    dataset = dataset.batch(batch_size)\n\n    # Prefetch for performance\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\n# ==============================================================================\n#                      MODEL ARCHITECTURE (Shallow & Wide 3-Branch)\n# ==============================================================================\n\ndef build_shallow_wide_3branch(seq_len, n_features_imu, n_features_thm, tof_shape, n_classes):\n    \"\"\"Defines the shallow-wide Three-Branch CNN-GRU model.\"\"\"\n    # Inputs\n    input_imu = keras.Input(shape=(seq_len, n_features_imu), name='imu_input')\n    input_thm = keras.Input(shape=(seq_len, n_features_thm), name='thm_input')\n    input_tof = keras.Input(shape=(seq_len,) + tof_shape, name='tof_input') # tof_shape = (8, 8, 5)\n\n    # --- IMU Branch (Wider 1D CNN) ---\n    x_imu = layers.Masking(mask_value=0.0)(input_imu) # Mask padding zeros\n    x_imu = layers.Conv1D(256, kernel_size=7, padding='same', activation='relu')(x_imu); x_imu = layers.BatchNormalization()(x_imu); x_imu = layers.SpatialDropout1D(CNN_SPATIAL_DROPOUT_RATE)(x_imu)\n    pool_size_1d = 4; x_imu = layers.MaxPooling1D(pool_size=pool_size_1d)(x_imu)\n\n    # --- THM Branch (Wider 1D CNN) ---\n    x_thm = layers.Masking(mask_value=0.0)(input_thm) # Mask padding zeros\n    x_thm = layers.Conv1D(128, kernel_size=7, padding='same', activation='relu')(x_thm); x_thm = layers.BatchNormalization()(x_thm); x_thm = layers.SpatialDropout1D(CNN_SPATIAL_DROPOUT_RATE)(x_thm)\n    x_thm = layers.MaxPooling1D(pool_size=pool_size_1d)(x_thm)\n\n    # --- TOF Branch (Wider 3D CNN) ---\n    mask_tof = layers.Masking(mask_value=0.0)(input_tof) # Mask padding zeros\n    x_tof = layers.Conv3D(128, kernel_size=(3, 3, 3), padding='same', activation='relu')(mask_tof); x_tof = layers.BatchNormalization()(x_tof); x_tof = layers.SpatialDropout3D(CNN_SPATIAL_DROPOUT_RATE)(x_tof)\n    pool_size_3d_time = 4; pool_size_3d_spatial = 2; x_tof = layers.MaxPooling3D(pool_size=(pool_size_3d_time, pool_size_3d_spatial, pool_size_3d_spatial))(x_tof)\n\n    # Calculate target shape statically for Reshape\n    reduced_seq_len_static = seq_len // pool_size_3d_time if seq_len is not None else None\n    reduced_dim1 = tof_shape[0] // pool_size_3d_spatial\n    reduced_dim2 = tof_shape[1] // pool_size_3d_spatial\n    num_filters_tof = 128\n    flattened_tof_features = reduced_dim1 * reduced_dim2 * num_filters_tof\n    reshape_target_tof = (reduced_seq_len_static, flattened_tof_features)\n    x_tof = layers.Reshape(reshape_target_tof)(x_tof)\n\n    # --- Fusion ---\n    x_concat = layers.Concatenate(axis=-1)([x_imu, x_thm, x_tof])\n\n    # --- GRU Layer ---\n    x = layers.Bidirectional(layers.GRU(256, dropout=LSTM_DROPOUT_RATE, recurrent_dropout=LSTM_RECURRENT_DROPOUT_RATE, return_sequences=False))(x_concat)\n\n    # --- Dense Head ---\n    x = layers.Dense(256, activation='relu')(x); x = layers.Dropout(DENSE_DROPOUT_RATE)(x)\n    output = layers.Dense(n_classes, activation='softmax', name='output')(x)\n\n    model = keras.Model(inputs=[input_imu, input_thm, input_tof], outputs=output)\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# ==============================================================================\n#                      COMPETITION METRIC FUNCTION (UNCHANGED)\n# ==============================================================================\ndef hierarchical_macro_f1(y_true, y_pred_labels, target_gestures_list, le):\n    \"\"\"Calculates the CMI competition metric with corrected variable scope.\"\"\"\n    # Ensure inputs are not empty first\n    if len(y_true) == 0 or len(y_pred_labels) == 0:\n        return 0.0, 0.0, 0.0\n\n    # --- Ensure assignments happen before potential use ---\n    y_true_np = np.asarray(y_true)\n    y_pred_labels_np = np.asarray(y_pred_labels)\n    # --- End Assignment Fix ---\n\n    # --- Check for valid label indices ---\n    known_labels = np.arange(len(le.classes_))\n    # Create mask where predicted labels are within the known range\n    valid_pred_mask = np.isin(y_pred_labels_np, known_labels)\n\n    # Filter both true and predicted labels based on the mask\n    y_true_filtered = y_true_np[valid_pred_mask]\n    y_pred_filtered = y_pred_labels_np[valid_pred_mask]\n\n    # If filtering removed all samples, return 0\n    if len(y_true_filtered) == 0:\n        return 0.0, 0.0, 0.0\n    # --- End Validity Check ---\n\n    # --- Proceed with metric calculation using filtered arrays ---\n    try:\n        y_true_str = le.inverse_transform(y_true_filtered)\n        y_pred_str = le.inverse_transform(y_pred_filtered)\n    except ValueError as e:\n        print(f\"LabelEncoder Error: {e}. Check label range. True: {np.unique(y_true_filtered)}, Pred: {np.unique(y_pred_filtered)}\")\n        return 0.0, 0.0, 0.0 # Return 0 if inverse_transform fails\n\n    y_true_bin = np.isin(y_true_str, target_gestures_list)\n    y_pred_bin = np.isin(y_pred_str, target_gestures_list)\n    binary_f1 = f1_score(y_true_bin, y_pred_bin, pos_label=True, zero_division=0)\n\n    y_true_mc = np.where(y_true_bin, y_true_str, 'non_target')\n    y_pred_mc = np.where(y_pred_bin, y_pred_str, 'non_target')\n    unique_labels_mc_present = np.unique(np.concatenate((y_true_mc, y_pred_mc)))\n\n    gesture_f1 = f1_score(y_true_mc, y_pred_mc, labels=unique_labels_mc_present, average='macro', zero_division=0)\n\n    metric = 0.5 * binary_f1 + 0.5 * gesture_f1\n    return metric, binary_f1, gesture_f1\n# ==============================================================================\n#                      CROSS-VALIDATION TRAINING\n# ==============================================================================\n\nprint(f\"\\nStarting {N_SPLITS}-Fold Cross-Validation (Shallow/Wide 3-Branch with Dropout)...\")\ngkf = GroupKFold(n_splits=N_SPLITS)\noof_preds = np.zeros((len(y), N_CLASSES))\noof_labels = np.zeros(len(y))\noof_groups_store = np.empty(len(y), dtype=object)\noof_indices = np.arange(len(y))\n\nall_histories = []\nfold_metrics = []\n\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X_imu, y, groups)): # Use X_imu shape for split\n    print(f\"\\n===== Fold {fold+1}/{N_SPLITS} =====\")\n    # Get NumPy data slices for this fold\n    X_train_imu, X_val_imu = X_imu[train_idx], X_imu[val_idx]\n    X_train_thm, X_val_thm = X_thm[train_idx], X_thm[val_idx]\n    X_train_tof, X_val_tof = X_tof[train_idx], X_tof[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    oof_groups_store[val_idx] = groups[val_idx] # Store subject IDs for OOF\n    oof_indices[val_idx] = val_idx\n\n    # Create tf.data datasets from NumPy arrays\n    train_dataset = create_tf_dataset_from_np(X_train_imu, X_train_thm, X_train_tof, y_train, BATCH_SIZE, shuffle=True, augment=True)\n    val_dataset = create_tf_dataset_from_np(X_val_imu, X_val_thm, X_val_tof, y_val, BATCH_SIZE, shuffle=False, augment=False)\n\n    keras.backend.clear_session()\n    tof_input_shape_model = (TOF_GRID_DIM, TOF_GRID_DIM, TOF_SENSORS)\n    try:\n        model = build_shallow_wide_3branch(MAX_LENGTH, N_FEATURES_IMU, N_FEATURES_THM, tof_input_shape_model, N_CLASSES)\n        if fold == 0: model.summary()\n    except Exception as e:\n        print(f\"Error building model: {e}\"); exit()\n\n    lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=0, min_lr=1e-6)\n    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1, mode='min')\n\n    print(f\"Training Fold {fold+1}...\")\n    try:\n        history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=[lr_scheduler, early_stopping], verbose=1)\n        all_histories.append(history)\n    except Exception as e:\n        print(f\"Error during model training for fold {fold+1}: {e}\"); continue\n\n    print(\"Predicting on validation set...\")\n    try:\n        # Predict directly on the validation dataset (which yields dictionary inputs)\n        fold_oof_preds = model.predict(val_dataset)\n    except Exception as e:\n        print(f\"Error predicting on validation dataset for fold {fold+1}: {e}\")\n        fold_oof_preds = np.full((len(val_idx), N_CLASSES), np.nan)\n\n    # Store predictions\n    if not np.isnan(fold_oof_preds).any():\n        oof_preds[val_idx] = fold_oof_preds\n        oof_labels[val_idx] = np.argmax(fold_oof_preds, axis=1)\n        metric, _, _ = hierarchical_macro_f1(y_val, np.argmax(fold_oof_preds, axis=1), TARGET_GESTURES, label_encoder)\n    else:\n        oof_labels[val_idx] = -1; metric = 0.0; print(\"Skipping metric due to prediction errors.\")\n\n    fold_metrics.append(metric)\n    print(f\"Fold {fold+1} Hierarchical F1: {metric:.4f}\")\n\n    # Save model\n    try: model.save(MODEL_TMPL.format(fold=fold+1)); print(f\"Model for fold {fold+1} saved.\")\n    except Exception as e: print(f\"Error saving model for fold {fold+1}: {e}\")\n\n    # Clean up\n    del model, train_dataset, val_dataset, history; gc.collect()\n    del X_train_imu, X_val_imu, X_train_thm, X_val_thm, X_train_tof, X_val_tof, y_train, y_val; gc.collect()\n\n\n# ==============================================================================\n#                      OVERALL OOF EVALUATION\n# ==============================================================================\nprint(\"\\n===== Overall OOF Evaluation (Shallow/Wide 3-Branch Dropout ) =====\")\nvalid_oof_mask_eval = oof_labels != -1\nif np.sum(valid_oof_mask_eval) > 0:\n    oof_metric_overall, oof_bin_f1, oof_gest_f1 = hierarchical_macro_f1(y[valid_oof_mask_eval], oof_labels.astype(int)[valid_oof_mask_eval], TARGET_GESTURES, label_encoder)\n    print(f\"Overall OOF Hierarchical F1 (on {np.sum(valid_oof_mask_eval)} samples): {oof_metric_overall:.4f}\")\n    print(f\"Mean Fold Metric: {np.mean([m for m in fold_metrics if m > 0]):.4f} +/- {np.std([m for m in fold_metrics if m > 0]):.4f}\")\n    print(\"\\nOOF Classification Report (Before PP):\")\n    print(classification_report(y[valid_oof_mask_eval], oof_labels.astype(int)[valid_oof_mask_eval], target_names=label_encoder.classes_, zero_division=0))\nelse: print(\"No valid OOF predictions available for evaluation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:38:17.097493Z","iopub.execute_input":"2025-10-23T16:38:17.098041Z","iopub.status.idle":"2025-10-23T17:03:12.203208Z","shell.execute_reply.started":"2025-10-23T16:38:17.098018Z","shell.execute_reply":"2025-10-23T17:03:12.202357Z"}},"outputs":[{"name":"stdout","text":"TensorFlow Version: 2.18.0\nLoading pre-trained objects and prepared NumPy arrays...\nUsing assumed scaled missing non-IMU indicator value: 0.0000\nLoaded NumPy arrays and found feature indices:\nX shape: (8151, 192, 338) (Expected features: 338)\ny shape: (8151,), groups shape: (8151,)\nIMU features: 13, THM features: 5, TOF features: 320\nNumber of classes: 18\n\nSplitting X into IMU, THM, TOF branches...\nSplit array shapes: X_imu=(8151, 192, 13), X_thm=(8151, 192, 5), X_tof=(8151, 192, 8, 8, 5)\n\nStarting 2-Fold Cross-Validation (Shallow/Wide 3-Branch with Dropout)...\n\n===== Fold 1/2 =====\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ tof_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m5\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ imu_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m13\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ thm_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m5\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masking_2 (\u001b[38;5;33mMasking\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, │          \u001b[38;5;34m0\u001b[0m │ tof_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m5\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masking (\u001b[38;5;33mMasking\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m13\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ imu_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masking_1 (\u001b[38;5;33mMasking\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m5\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ thm_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, │     \u001b[38;5;34m17,408\u001b[0m │ masking_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m23,552\u001b[0m │ masking[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │      \u001b[38;5;34m4,608\u001b[0m │ masking_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, │        \u001b[38;5;34m512\u001b[0m │ conv3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spatial_dropout3d   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mSpatialDropout3D\u001b[0m)  │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spatial_dropout1d   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spatial_dropout1d_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling3d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ spatial_dropout3… │\n│ (\u001b[38;5;33mMaxPooling3D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ spatial_dropout1… │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ spatial_dropout1… │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ max_pooling3d[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m2432\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │  \u001b[38;5;34m4,131,840\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │      \u001b[38;5;34m4,626\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ tof_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ imu_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ thm_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masking_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ tof_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ imu_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masking_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ thm_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, │     <span style=\"color: #00af00; text-decoration-color: #00af00\">17,408</span> │ masking_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,552</span> │ masking[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,608</span> │ masking_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spatial_dropout3d   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout3D</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spatial_dropout1d   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ spatial_dropout1d_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling3d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ spatial_dropout3… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ spatial_dropout1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ spatial_dropout1… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2432</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,131,840</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,626</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,315,410\u001b[0m (16.46 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,315,410</span> (16.46 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,314,386\u001b[0m (16.46 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,314,386</span> (16.46 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,024\u001b[0m (4.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> (4.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Training Fold 1...\nEpoch 1/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 517ms/step - accuracy: 0.0992 - loss: 2.9658 - val_accuracy: 0.1629 - val_loss: 2.6044 - learning_rate: 5.0000e-04\nEpoch 2/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.1605 - loss: 2.5635 - val_accuracy: 0.1803 - val_loss: 2.5048 - learning_rate: 5.0000e-04\nEpoch 3/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.1756 - loss: 2.4580 - val_accuracy: 0.1884 - val_loss: 2.4901 - learning_rate: 5.0000e-04\nEpoch 4/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.1985 - loss: 2.3675 - val_accuracy: 0.1906 - val_loss: 2.4433 - learning_rate: 5.0000e-04\nEpoch 5/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.2260 - loss: 2.2672 - val_accuracy: 0.2102 - val_loss: 2.3452 - learning_rate: 5.0000e-04\nEpoch 6/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.2337 - loss: 2.2183 - val_accuracy: 0.2381 - val_loss: 2.2815 - learning_rate: 5.0000e-04\nEpoch 7/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.2618 - loss: 2.1323 - val_accuracy: 0.2577 - val_loss: 2.2041 - learning_rate: 5.0000e-04\nEpoch 8/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.2771 - loss: 2.0684 - val_accuracy: 0.2822 - val_loss: 2.1260 - learning_rate: 5.0000e-04\nEpoch 9/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.2823 - loss: 2.0207 - val_accuracy: 0.3013 - val_loss: 2.0344 - learning_rate: 5.0000e-04\nEpoch 10/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.2934 - loss: 1.9918 - val_accuracy: 0.3322 - val_loss: 1.9133 - learning_rate: 5.0000e-04\nEpoch 11/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.3182 - loss: 1.9012 - val_accuracy: 0.3373 - val_loss: 1.8808 - learning_rate: 5.0000e-04\nEpoch 12/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.3336 - loss: 1.8956 - val_accuracy: 0.3503 - val_loss: 1.8607 - learning_rate: 5.0000e-04\nEpoch 13/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.3366 - loss: 1.8534 - val_accuracy: 0.3677 - val_loss: 1.8213 - learning_rate: 5.0000e-04\nEpoch 14/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.3569 - loss: 1.8076 - val_accuracy: 0.3827 - val_loss: 1.7863 - learning_rate: 5.0000e-04\nEpoch 15/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.3533 - loss: 1.8067 - val_accuracy: 0.3920 - val_loss: 1.7454 - learning_rate: 5.0000e-04\nEpoch 16/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.3648 - loss: 1.7579 - val_accuracy: 0.3915 - val_loss: 1.7238 - learning_rate: 5.0000e-04\nEpoch 17/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.3631 - loss: 1.7544 - val_accuracy: 0.3677 - val_loss: 1.7443 - learning_rate: 5.0000e-04\nEpoch 18/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.3863 - loss: 1.6886 - val_accuracy: 0.3993 - val_loss: 1.7182 - learning_rate: 5.0000e-04\nEpoch 19/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4020 - loss: 1.6550 - val_accuracy: 0.4052 - val_loss: 1.7052 - learning_rate: 5.0000e-04\nEpoch 20/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.3980 - loss: 1.6561 - val_accuracy: 0.4049 - val_loss: 1.6750 - learning_rate: 5.0000e-04\nEpoch 21/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.3931 - loss: 1.6376 - val_accuracy: 0.4258 - val_loss: 1.6551 - learning_rate: 5.0000e-04\nEpoch 22/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4127 - loss: 1.6120 - val_accuracy: 0.4209 - val_loss: 1.6437 - learning_rate: 5.0000e-04\nEpoch 23/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4195 - loss: 1.5729 - val_accuracy: 0.4272 - val_loss: 1.6308 - learning_rate: 5.0000e-04\nEpoch 24/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4172 - loss: 1.5951 - val_accuracy: 0.4361 - val_loss: 1.6064 - learning_rate: 5.0000e-04\nEpoch 25/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 452ms/step - accuracy: 0.4303 - loss: 1.5216 - val_accuracy: 0.4196 - val_loss: 1.6791 - learning_rate: 5.0000e-04\nEpoch 26/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 452ms/step - accuracy: 0.4263 - loss: 1.5267 - val_accuracy: 0.4392 - val_loss: 1.5765 - learning_rate: 5.0000e-04\nEpoch 27/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4317 - loss: 1.5095 - val_accuracy: 0.4363 - val_loss: 1.5629 - learning_rate: 5.0000e-04\nEpoch 28/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.4545 - loss: 1.4410 - val_accuracy: 0.4417 - val_loss: 1.5996 - learning_rate: 5.0000e-04\nEpoch 29/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4450 - loss: 1.4779 - val_accuracy: 0.4245 - val_loss: 1.6248 - learning_rate: 5.0000e-04\nEpoch 30/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4412 - loss: 1.4846 - val_accuracy: 0.4427 - val_loss: 1.5488 - learning_rate: 5.0000e-04\nEpoch 31/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4633 - loss: 1.4494 - val_accuracy: 0.4554 - val_loss: 1.5498 - learning_rate: 5.0000e-04\nEpoch 32/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4837 - loss: 1.3905 - val_accuracy: 0.4613 - val_loss: 1.5346 - learning_rate: 5.0000e-04\nEpoch 33/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4648 - loss: 1.4373 - val_accuracy: 0.4510 - val_loss: 1.5408 - learning_rate: 5.0000e-04\nEpoch 34/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4620 - loss: 1.4151 - val_accuracy: 0.4826 - val_loss: 1.4849 - learning_rate: 5.0000e-04\nEpoch 35/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4816 - loss: 1.3685 - val_accuracy: 0.4740 - val_loss: 1.4664 - learning_rate: 5.0000e-04\nEpoch 36/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4869 - loss: 1.3711 - val_accuracy: 0.4895 - val_loss: 1.4889 - learning_rate: 5.0000e-04\nEpoch 37/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.4743 - loss: 1.3748 - val_accuracy: 0.4672 - val_loss: 1.5265 - learning_rate: 5.0000e-04\nEpoch 38/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4833 - loss: 1.3350 - val_accuracy: 0.4851 - val_loss: 1.4718 - learning_rate: 5.0000e-04\nEpoch 39/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4773 - loss: 1.3570 - val_accuracy: 0.4858 - val_loss: 1.4851 - learning_rate: 5.0000e-04\nEpoch 40/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4874 - loss: 1.3435 - val_accuracy: 0.4973 - val_loss: 1.4487 - learning_rate: 2.5000e-04\nEpoch 41/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.5154 - loss: 1.2802 - val_accuracy: 0.4912 - val_loss: 1.4760 - learning_rate: 2.5000e-04\nEpoch 42/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.5143 - loss: 1.2962 - val_accuracy: 0.4924 - val_loss: 1.4630 - learning_rate: 2.5000e-04\nEpoch 43/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.5248 - loss: 1.2578 - val_accuracy: 0.4949 - val_loss: 1.4534 - learning_rate: 2.5000e-04\nEpoch 44/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.5281 - loss: 1.2425 - val_accuracy: 0.5029 - val_loss: 1.4322 - learning_rate: 2.5000e-04\nEpoch 45/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.5161 - loss: 1.2638 - val_accuracy: 0.5054 - val_loss: 1.4369 - learning_rate: 2.5000e-04\nEpoch 46/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.5274 - loss: 1.2398 - val_accuracy: 0.5059 - val_loss: 1.4395 - learning_rate: 2.5000e-04\nEpoch 47/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.5205 - loss: 1.2377 - val_accuracy: 0.5091 - val_loss: 1.4359 - learning_rate: 2.5000e-04\nEpoch 48/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.5290 - loss: 1.2186 - val_accuracy: 0.5049 - val_loss: 1.4398 - learning_rate: 2.5000e-04\nEpoch 49/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.5357 - loss: 1.2179 - val_accuracy: 0.5110 - val_loss: 1.4058 - learning_rate: 1.2500e-04\nEpoch 50/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 452ms/step - accuracy: 0.5449 - loss: 1.1709 - val_accuracy: 0.5157 - val_loss: 1.4079 - learning_rate: 1.2500e-04\nRestoring model weights from the end of the best epoch: 49.\nPredicting on validation set...\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 143ms/step\nFold 1 Hierarchical F1: 0.7025\nModel for fold 1 saved.\n\n===== Fold 2/2 =====\nTraining Fold 2...\nEpoch 1/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 519ms/step - accuracy: 0.0948 - loss: 2.9294 - val_accuracy: 0.1521 - val_loss: 2.5572 - learning_rate: 5.0000e-04\nEpoch 2/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.1508 - loss: 2.5480 - val_accuracy: 0.1219 - val_loss: 2.5876 - learning_rate: 5.0000e-04\nEpoch 3/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.1800 - loss: 2.4623 - val_accuracy: 0.1381 - val_loss: 2.5175 - learning_rate: 5.0000e-04\nEpoch 4/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.2022 - loss: 2.3675 - val_accuracy: 0.1384 - val_loss: 2.4865 - learning_rate: 5.0000e-04\nEpoch 5/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.2072 - loss: 2.3191 - val_accuracy: 0.1978 - val_loss: 2.4224 - learning_rate: 5.0000e-04\nEpoch 6/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.2308 - loss: 2.2245 - val_accuracy: 0.2413 - val_loss: 2.2949 - learning_rate: 5.0000e-04\nEpoch 7/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.2466 - loss: 2.2166 - val_accuracy: 0.2848 - val_loss: 2.2121 - learning_rate: 5.0000e-04\nEpoch 8/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.2569 - loss: 2.1530 - val_accuracy: 0.2829 - val_loss: 2.1116 - learning_rate: 5.0000e-04\nEpoch 9/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.2714 - loss: 2.1284 - val_accuracy: 0.2986 - val_loss: 2.0260 - learning_rate: 5.0000e-04\nEpoch 10/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.2714 - loss: 2.0903 - val_accuracy: 0.3116 - val_loss: 2.0007 - learning_rate: 5.0000e-04\nEpoch 11/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.3076 - loss: 2.0225 - val_accuracy: 0.3347 - val_loss: 1.9593 - learning_rate: 5.0000e-04\nEpoch 12/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.2841 - loss: 2.0043 - val_accuracy: 0.3527 - val_loss: 1.8542 - learning_rate: 5.0000e-04\nEpoch 13/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.3034 - loss: 1.9876 - val_accuracy: 0.3740 - val_loss: 1.8022 - learning_rate: 5.0000e-04\nEpoch 14/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.3191 - loss: 1.9408 - val_accuracy: 0.3989 - val_loss: 1.7431 - learning_rate: 5.0000e-04\nEpoch 15/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.3363 - loss: 1.8727 - val_accuracy: 0.4006 - val_loss: 1.7240 - learning_rate: 5.0000e-04\nEpoch 16/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.3644 - loss: 1.8061 - val_accuracy: 0.3893 - val_loss: 1.7142 - learning_rate: 5.0000e-04\nEpoch 17/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 452ms/step - accuracy: 0.3341 - loss: 1.8310 - val_accuracy: 0.4350 - val_loss: 1.6311 - learning_rate: 5.0000e-04\nEpoch 18/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.3522 - loss: 1.8219 - val_accuracy: 0.4109 - val_loss: 1.6729 - learning_rate: 5.0000e-04\nEpoch 19/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 452ms/step - accuracy: 0.3439 - loss: 1.7982 - val_accuracy: 0.4397 - val_loss: 1.5586 - learning_rate: 5.0000e-04\nEpoch 20/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.3746 - loss: 1.7460 - val_accuracy: 0.4279 - val_loss: 1.5515 - learning_rate: 5.0000e-04\nEpoch 21/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.4001 - loss: 1.6917 - val_accuracy: 0.4451 - val_loss: 1.5169 - learning_rate: 5.0000e-04\nEpoch 22/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.3796 - loss: 1.7121 - val_accuracy: 0.4615 - val_loss: 1.4786 - learning_rate: 5.0000e-04\nEpoch 23/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4051 - loss: 1.6634 - val_accuracy: 0.4451 - val_loss: 1.4822 - learning_rate: 5.0000e-04\nEpoch 24/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4097 - loss: 1.6315 - val_accuracy: 0.4524 - val_loss: 1.4794 - learning_rate: 5.0000e-04\nEpoch 25/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4054 - loss: 1.6184 - val_accuracy: 0.4505 - val_loss: 1.4670 - learning_rate: 5.0000e-04\nEpoch 26/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4129 - loss: 1.5821 - val_accuracy: 0.4662 - val_loss: 1.4476 - learning_rate: 5.0000e-04\nEpoch 27/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.4265 - loss: 1.5640 - val_accuracy: 0.4832 - val_loss: 1.4328 - learning_rate: 5.0000e-04\nEpoch 28/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4226 - loss: 1.5450 - val_accuracy: 0.4701 - val_loss: 1.4098 - learning_rate: 5.0000e-04\nEpoch 29/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.4257 - loss: 1.5255 - val_accuracy: 0.4709 - val_loss: 1.4007 - learning_rate: 5.0000e-04\nEpoch 30/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 451ms/step - accuracy: 0.4418 - loss: 1.4984 - val_accuracy: 0.4928 - val_loss: 1.3827 - learning_rate: 5.0000e-04\nEpoch 31/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.4289 - loss: 1.5223 - val_accuracy: 0.4785 - val_loss: 1.3985 - learning_rate: 5.0000e-04\nEpoch 32/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4459 - loss: 1.4830 - val_accuracy: 0.4841 - val_loss: 1.3669 - learning_rate: 5.0000e-04\nEpoch 33/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4901 - loss: 1.4128 - val_accuracy: 0.5173 - val_loss: 1.2928 - learning_rate: 5.0000e-04\nEpoch 41/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4759 - loss: 1.3850 - val_accuracy: 0.5156 - val_loss: 1.2695 - learning_rate: 5.0000e-04\nEpoch 42/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4985 - loss: 1.3750 - val_accuracy: 0.5358 - val_loss: 1.2548 - learning_rate: 5.0000e-04\nEpoch 43/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4854 - loss: 1.3545 - val_accuracy: 0.5161 - val_loss: 1.2888 - learning_rate: 5.0000e-04\nEpoch 44/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.4926 - loss: 1.3737 - val_accuracy: 0.5215 - val_loss: 1.2707 - learning_rate: 5.0000e-04\nEpoch 45/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.4874 - loss: 1.3568 - val_accuracy: 0.5141 - val_loss: 1.2876 - learning_rate: 5.0000e-04\nEpoch 46/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4957 - loss: 1.3416 - val_accuracy: 0.5112 - val_loss: 1.2671 - learning_rate: 5.0000e-04\nEpoch 47/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.4998 - loss: 1.3285 - val_accuracy: 0.5328 - val_loss: 1.2320 - learning_rate: 2.5000e-04\nEpoch 48/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 448ms/step - accuracy: 0.5073 - loss: 1.2937 - val_accuracy: 0.5362 - val_loss: 1.2331 - learning_rate: 2.5000e-04\nEpoch 49/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 449ms/step - accuracy: 0.5145 - loss: 1.2685 - val_accuracy: 0.5419 - val_loss: 1.2215 - learning_rate: 2.5000e-04\nEpoch 50/50\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 450ms/step - accuracy: 0.5068 - loss: 1.2911 - val_accuracy: 0.5392 - val_loss: 1.2267 - learning_rate: 2.5000e-04\nRestoring model weights from the end of the best epoch: 49.\nPredicting on validation set...\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 141ms/step\nFold 2 Hierarchical F1: 0.7278\nModel for fold 2 saved.\n\n===== Overall OOF Evaluation (Shallow/Wide 3-Branch Dropout ) =====\nOverall OOF Hierarchical F1 (on 8151 samples): 0.7155\nMean Fold Metric: 0.7152 +/- 0.0126\n\nOOF Classification Report (Before PP):\n                                            precision    recall  f1-score   support\n\n                     Above ear - pull hair       0.56      0.70      0.62       638\n                        Cheek - pinch skin       0.35      0.48      0.40       637\n                     Drink from bottle/cup       0.86      0.73      0.79       161\n                       Eyebrow - pull hair       0.34      0.18      0.23       638\n                       Eyelash - pull hair       0.35      0.37      0.36       640\nFeel around in tray and pull out an object       0.89      0.89      0.89       161\n                  Forehead - pull hairline       0.56      0.32      0.41       640\n                        Forehead - scratch       0.46      0.63      0.53       640\n                            Glasses on/off       0.83      0.65      0.73       161\n                         Neck - pinch skin       0.38      0.42      0.40       640\n                            Neck - scratch       0.42      0.36      0.39       640\n                       Pinch knee/leg skin       0.29      0.35      0.32       161\n                 Pull air toward your face       0.77      0.82      0.79       477\n                     Scratch knee/leg skin       0.43      0.25      0.32       161\n                             Text on phone       0.81      0.83      0.82       640\n                                Wave hello       0.72      0.69      0.71       478\n                         Write name in air       0.65      0.63      0.64       477\n                         Write name on leg       0.36      0.38      0.37       161\n\n                                  accuracy                           0.53      8151\n                                 macro avg       0.56      0.54      0.54      8151\n                              weighted avg       0.53      0.53      0.52      8151\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}