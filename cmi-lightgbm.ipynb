{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder \nfrom scipy.spatial.transform import Rotation as R\nimport joblib\nimport gc\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nDATA_DIR = '/kaggle/input/cmi-detect-behavior-with-sensor-data'\nTRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\nTRAIN_DEMO_CSV = os.path.join(DATA_DIR, 'train_demographics.csv')\nTOF_FILL_VALUE = 500 # Value to replace -1 in TOF\n\n# --- Output Paths ---\nOUTPUT_DIR = '/kaggle/working/data/preprocessed' # Save outputs in working directory subfolder\nSCALER_PATH = os.path.join(OUTPUT_DIR, 'standard_scaler.joblib')\nLABEL_ENCODER_PATH = os.path.join(OUTPUT_DIR, 'label_encoder.joblib')\nIMU_COLS_PATH = os.path.join(OUTPUT_DIR, 'imu_feature_cols_3branch.pkl')\nTHM_COLS_PATH = os.path.join(OUTPUT_DIR, 'thm_feature_cols_3branch.pkl')\nTOF_COLS_PATH = os.path.join(OUTPUT_DIR, 'tof_feature_cols_3branch.pkl')\nALL_COLS_PATH = os.path.join(OUTPUT_DIR, 'all_feature_cols_3branch.pkl')\nPROCESSED_TRAIN_PATH = os.path.join(OUTPUT_DIR, 'train_processed.parquet') # Save processed data\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Output directory created/exists at: {OUTPUT_DIR}\")\n\n\n# ==============================================================================\n#                 HELPER FUNCTIONS (Preprocessing Steps)\n# ==============================================================================\n\n# --- Define Sensor Columns ---\n# Define these globally or pass them around as needed\nacc_cols = ['acc_x', 'acc_y', 'acc_z']\nrot_cols = ['rot_x', 'rot_y', 'rot_z', 'rot_w']\n# Dynamically get thm/tof cols after loading\ntry:\n    _temp_df = pd.read_csv(TRAIN_CSV, nrows=0) # Read only header\n    thm_cols = sorted([col for col in _temp_df.columns if 'thm_' in col])\n    tof_cols = sorted([col for col in _temp_df.columns if 'tof_' in col])\n    del _temp_df\nexcept Exception as e:\n    print(f\"Warning: Could not dynamically determine thm/tof columns from header: {e}\")\n    # Fallback to static definition if dynamic fails\n    thm_cols = ['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5']\n    tof_cols = sorted([f'tof_{s}_v{i}' for s in range(1, 6) for i in range(64)])\n\n\nimu_cols = acc_cols + rot_cols\nnon_imu_sensor_cols = thm_cols + tof_cols\nall_initial_sensor_cols = imu_cols + non_imu_sensor_cols\n\nTARGET_GESTURES = [\n    'Above ear - pull hair', 'Cheek - pinch skin', 'Eyebrow - pull hair',\n    'Eyelash - pull hair', 'Forehead - pull hairline', 'Forehead - scratch',\n    'Neck - pinch skin', 'Neck - scratch'\n]\n\ndef preprocess_impute(df, tof_fill_value=500):\n    \"\"\"Handles imputation for sensor data.\"\"\"\n    print(f\"Imputing with TOF fill value: {tof_fill_value}...\")\n    present_tof_cols = [col for col in tof_cols if col in df.columns]\n    present_thm_cols = [col for col in thm_cols if col in df.columns]\n    present_all_initial_cols = [col for col in all_initial_sensor_cols if col in df.columns]\n\n    if present_tof_cols:\n        df[present_tof_cols] = df[present_tof_cols].replace(-1, tof_fill_value)\n    if present_thm_cols:\n        for col in present_thm_cols:\n             df[col] = df[col].apply(lambda x: np.nan if x < 20 else x)\n        if 'sequence_id' in df.columns:\n            df[present_thm_cols] = df.groupby('sequence_id')[present_thm_cols].transform(\n                lambda x: x.interpolate(method='linear', limit_direction='both', axis=0)\n            )\n        else:\n            df[present_thm_cols] = df[present_thm_cols].interpolate(method='linear', limit_direction='both', axis=0)\n\n    group_cols_to_fill = list(set(present_all_initial_cols) - set(present_thm_cols))\n    if 'sequence_id' in df.columns and group_cols_to_fill:\n         df[group_cols_to_fill] = df.groupby('sequence_id')[group_cols_to_fill].transform(\n             lambda x: x.ffill().bfill()\n         )\n    elif group_cols_to_fill:\n         df[group_cols_to_fill] = df[group_cols_to_fill].ffill().bfill()\n\n    if present_all_initial_cols:\n        df[present_all_initial_cols] = df[present_all_initial_cols].fillna(0)\n    print(\"Imputation complete.\")\n    return df\n\ndef correct_handedness(df, demo_df):\n    \"\"\"Corrects sensor readings based on subject handedness.\"\"\"\n    print(\"Correcting handedness...\")\n    if 'handedness' not in df.columns:\n      if 'subject' in df.columns and 'subject' in demo_df.columns:\n          df['subject'] = df['subject'].astype(str)\n          demo_df['subject'] = demo_df['subject'].astype(str)\n          df = df.merge(demo_df[['subject', 'handedness']], on='subject', how='left')\n          df['handedness'] = df['handedness'].fillna('Right')\n      else:\n          df['handedness'] = 'Right'\n\n    left_handed_mask = df['handedness'] == 'Left'\n    if left_handed_mask.any():\n        df.loc[left_handed_mask, 'acc_x'] *= -1\n        df.loc[left_handed_mask, 'rot_y'] *= -1\n        df.loc[left_handed_mask, 'rot_z'] *= -1\n    print(\"Handedness correction complete.\")\n    return df\n\ndef correct_upside_down(df):\n    \"\"\"Corrects sensor readings for known upside-down subjects.\"\"\"\n    print(\"Correcting upside-down subjects...\")\n    upside_down_subjects = ['SUBJ_019262', 'SUBJ_045235']\n    if 'subject' in df.columns:\n        df['subject'] = df['subject'].astype(str)\n        ud_mask = df['subject'].isin(upside_down_subjects)\n        if ud_mask.any():\n            df.loc[ud_mask, ['acc_x', 'acc_y']] *= -1\n            df.loc[ud_mask, ['rot_x', 'rot_y']] *= -1\n    print(\"Upside-down correction complete.\")\n    return df\n\ndef add_linear_acceleration(df):\n    \"\"\"Calculates linear acceleration by removing gravity.\"\"\"\n    print(\"Calculating linear acceleration...\")\n    if not all(col in df.columns for col in rot_cols + acc_cols):\n        print(\"Warning: Acc/Rot columns missing. Assigning raw acceleration to linear.\")\n        # Ensure target columns exist before assignment\n        for col in acc_cols:\n             if col not in df.columns: df[col] = 0.0 # Add missing acc cols if needed\n        df['lin_acc_x'], df['lin_acc_y'], df['lin_acc_z'] = df['acc_x'], df['acc_y'], df['acc_z']\n        return df\n\n    # Ensure float type before processing\n    for col in rot_cols + acc_cols:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n\n    quats = df[rot_cols].values\n    accels = df[acc_cols].values\n    linear_accel = np.zeros_like(accels)\n    gravity_world = np.array([0, 0, 1.0])\n\n    quats_numeric = np.nan_to_num(quats)\n    valid_quat_mask = ~np.all(quats_numeric == 0, axis=1) & (np.abs(np.linalg.norm(quats_numeric, axis=1) - 1.0) < 1e-2)\n    valid_quats_data = quats_numeric[valid_quat_mask]\n\n    if len(valid_quats_data) > 0:\n        try:\n            norms = np.linalg.norm(valid_quats_data, axis=1, keepdims=True)\n            norms[norms < 1e-6] = 1.0\n            valid_quats_normalized = valid_quats_data / norms\n            valid_quats_normalized = np.clip(valid_quats_normalized, -1.0, 1.0)\n            r = R.from_quat(valid_quats_normalized)\n            r_inv = r.inv()\n            gravity_sensor_frame = r_inv.apply(gravity_world)\n            linear_accel[valid_quat_mask] = accels[valid_quat_mask] - gravity_sensor_frame\n        except Exception as e:\n            print(f\"Warning: Scipy Rotation error during gravity removal: {e}. Using raw accel for some rows.\")\n            linear_accel[valid_quat_mask] = accels[valid_quat_mask] # Fallback\n\n    invalid_quat_mask = ~valid_quat_mask\n    linear_accel[invalid_quat_mask] = accels[invalid_quat_mask] # Use raw accel if quat invalid\n\n    df['lin_acc_x'], df['lin_acc_y'], df['lin_acc_z'] = linear_accel[:, 0], linear_accel[:, 1], linear_accel[:, 2]\n    print(\"Linear acceleration added.\")\n    return df\n\ndef add_basic_features(df):\n    \"\"\"Adds magnitude features.\"\"\"\n    print(\"Adding basic features (magnitudes)...\")\n     # Ensure float type\n    acc_mag_cols = ['acc_x', 'acc_y', 'acc_z']\n    lin_acc_mag_cols = ['lin_acc_x', 'lin_acc_y', 'lin_acc_z']\n    rot_mag_cols = ['rot_x', 'rot_y', 'rot_z']\n    for col in acc_mag_cols + lin_acc_mag_cols + rot_mag_cols:\n         if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n\n    if all(col in df.columns for col in acc_mag_cols):\n        df['acc_mag'] = np.linalg.norm(df[acc_mag_cols].values, axis=1)\n    else: df['acc_mag'] = 0.0\n\n    if all(col in df.columns for col in lin_acc_mag_cols):\n        df['lin_acc_mag'] = np.linalg.norm(df[lin_acc_mag_cols].values, axis=1)\n    else: df['lin_acc_mag'] = 0.0\n\n    if all(col in df.columns for col in rot_mag_cols):\n        df['rot_mag'] = np.linalg.norm(df[rot_mag_cols].values, axis=1)\n    else: df['rot_mag'] = 0.0\n    print(\"Basic features added.\")\n    return df\n\ndef full_preprocess_pipeline(df, demo_df, scaler=None, fit_scaler=False):\n    \"\"\"Applies the full preprocessing pipeline using CPU libraries.\"\"\"\n    print(\"-\" * 30)\n    print(\"Starting Full Preprocessing Pipeline...\")\n    # Ensure IDs are numeric/string\n    if 'sequence_id' in df.columns and df['sequence_id'].dtype == 'object':\n         df['sequence_id'] = df['sequence_id'].str.extract('(\\d+)').astype(int)\n    if 'subject' in df.columns: df['subject'] = df['subject'].astype(str)\n    if 'subject' in demo_df.columns: demo_df['subject'] = demo_df['subject'].astype(str)\n\n    # --- Preprocessing Steps ---\n    df_processed = preprocess_impute(df.copy(), tof_fill_value=TOF_FILL_VALUE)\n    df_processed = correct_handedness(df_processed, demo_df)\n    df_processed = correct_upside_down(df_processed)\n    df_processed = add_linear_acceleration(df_processed)\n    df_processed = add_basic_features(df_processed)\n\n    # --- Scaling ---\n    engineered_imu_cols = ['lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'acc_mag', 'lin_acc_mag', 'rot_mag']\n    potential_cols_to_scale = list(set(imu_cols + non_imu_sensor_cols + engineered_imu_cols))\n    # Ensure columns exist and sort for consistent order\n    cols_to_scale = sorted([col for col in potential_cols_to_scale if col in df_processed.columns])\n\n    if not cols_to_scale:\n         print(\"Warning: No columns identified for scaling.\")\n         return df_processed, scaler, cols_to_scale # Return early if no cols\n\n    if fit_scaler:\n        print(f\"Fitting StandardScaler on {len(cols_to_scale)} features.\")\n        scaler = StandardScaler()\n        # Ensure data is numeric before fitting\n        df_processed[cols_to_scale] = df_processed[cols_to_scale].apply(pd.to_numeric, errors='coerce').fillna(0)\n        df_processed[cols_to_scale] = scaler.fit_transform(df_processed[cols_to_scale])\n        # Store feature names IN THE ORDER THEY WERE FITTED\n        # Use get_feature_names_out if available, otherwise use cols_to_scale\n        try:\n             scaler.feature_names_in_ = scaler.get_feature_names_out()\n        except AttributeError:\n             scaler.feature_names_in_ = cols_to_scale\n        print(\"Scaler fitted.\")\n    elif scaler is not None:\n        print(f\"Transforming features using loaded scaler...\")\n        # Ensure scaler has feature names\n        if not hasattr(scaler, 'feature_names_in_') or scaler.feature_names_in_ is None:\n             raise ValueError(\"Loaded scaler is missing 'feature_names_in_'. Cannot proceed.\")\n\n        # Ensure columns match scaler's expectations\n        cols_available_ordered = [col for col in scaler.feature_names_in_ if col in df_processed.columns]\n        # Create a DataFrame with the exact columns and order the scaler expects\n        df_ordered = pd.DataFrame(0.0, index=df_processed.index, columns=scaler.feature_names_in_)\n        # Fill with available data, respecting scaler's column order\n        df_ordered[cols_available_ordered] = df_processed[cols_available_ordered]\n        if len(cols_available_ordered) < len(scaler.feature_names_in_):\n            missing_cols = list(set(scaler.feature_names_in_) - set(cols_available_ordered))\n            print(f\"Warning: Filled {len(missing_cols)} missing columns with zeros before scaling: {missing_cols}\")\n        # Ensure data is numeric before transforming\n        df_ordered[scaler.feature_names_in_] = df_ordered[scaler.feature_names_in_].apply(pd.to_numeric, errors='coerce').fillna(0)\n        # Transform using the reordered/filled dataframe\n        df_processed[scaler.feature_names_in_] = scaler.transform(df_ordered[scaler.feature_names_in_])\n        print(\"Transformation complete.\")\n    else:\n        raise ValueError(\"Scaler must be provided if fit_scaler is False\")\n\n    # Drop handedness column after scaling (if it wasn't used as feature)\n    if 'handedness' in df_processed.columns and 'handedness' not in cols_to_scale:\n        df_processed = df_processed.drop(columns=['handedness'])\n\n    print(\"Full Preprocessing Pipeline Finished.\")\n    print(\"-\" * 30)\n    return df_processed, scaler, cols_to_scale\n\n# ==============================================================================\n#                      MAIN EXECUTION (Preprocessing Only)\n# ==============================================================================\nprint(\"Running Preprocessing Script...\")\n\n# --- Load Data ---\nprint(\"Loading data...\")\ntry:\n    train_df_main = pd.read_csv(TRAIN_CSV)\n    train_demo_df_main = pd.read_csv(TRAIN_DEMO_CSV)\n    print(f\"Loaded Train shape: {train_df_main.shape}\")\nexcept FileNotFoundError:\n    print(f\"Error: Training files not found at {DATA_DIR}. Exiting.\")\n    exit()\nexcept Exception as e:\n    print(f\"Error loading data: {e}. Exiting.\")\n    exit()\n\n# --- Apply Full Preprocessing to Training Data ---\ntrain_processed_df_main, scaler_main, feature_columns_main = full_preprocess_pipeline(\n    train_df_main, train_demo_df_main, scaler=None, fit_scaler=True\n)\nprint(f\"Processed Train shape: {train_processed_df_main.shape}\")\nprint(f\"Number of features scaled: {len(feature_columns_main)}\")\n\n# --- Define Feature Lists for Branches (Example) ---\nimu_eng_cols_main = ['lin_acc_x', 'lin_acc_y', 'lin_acc_z', 'acc_mag', 'lin_acc_mag', 'rot_mag']\nmodel_imu_feature_cols_main = sorted([col for col in feature_columns_main if col in imu_cols + imu_eng_cols_main])\nmodel_thm_feature_cols_main = sorted([col for col in feature_columns_main if col in thm_cols])\nmodel_tof_feature_cols_main = sorted([col for col in feature_columns_main if col in tof_cols])\n\nN_FEATURES_IMU_main = len(model_imu_feature_cols_main)\nN_FEATURES_THM_main = len(model_thm_feature_cols_main)\nN_FEATURES_TOF_main = len(model_tof_feature_cols_main)\nprint(f\"Branch Features: IMU={N_FEATURES_IMU_main}, THM={N_FEATURES_THM_main}, TOF={N_FEATURES_TOF_main}\")\n\n\n# --- Save Objects ---\nprint(\"\\nSaving preprocessor objects...\")\njoblib.dump(scaler_main, SCALER_PATH)\njoblib.dump(model_imu_feature_cols_main, IMU_COLS_PATH)\njoblib.dump(model_thm_feature_cols_main, THM_COLS_PATH)\njoblib.dump(model_tof_feature_cols_main, TOF_COLS_PATH)\njoblib.dump(feature_columns_main, ALL_COLS_PATH) # Save list of all scaled features\nprint(f\"Objects saved to: {OUTPUT_DIR}\")\n\n# --- Label Encode and Save Encoder ---\nprint(\"Encoding target and saving encoder...\")\nlabel_encoder_main = LabelEncoder()\nif 'gesture' in train_processed_df_main.columns:\n    train_processed_df_main['gesture_encoded'] = label_encoder_main.fit_transform(train_processed_df_main['gesture'])\n    joblib.dump(label_encoder_main, LABEL_ENCODER_PATH)\n    print(f\"Target Classes ({len(label_encoder_main.classes_)}): {label_encoder_main.classes_}\")\n    print(f\"Label encoder saved to: {LABEL_ENCODER_PATH}\")\nelse:\n    print(\"Warning: 'gesture' column not found in processed data. Cannot save label encoder.\")\n\n# --- Save Processed Data (Optional) ---\nprint(f\"\\nSaving processed training data to {PROCESSED_TRAIN_PATH}...\")\ntry:\n    # Ensure directory exists before saving\n    os.makedirs(os.path.dirname(PROCESSED_TRAIN_PATH), exist_ok=True)\n    train_processed_df_main.to_parquet(PROCESSED_TRAIN_PATH, index=False)\n    print(\"Processed data saved.\")\nexcept Exception as e:\n    print(f\"Error saving processed data: {e}\")\n\nprint(\"\\nPreprocessing script finished.\")\n\n# Clean up large dataframes from memory\ndel train_df_main, train_demo_df_main, train_processed_df_main\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T08:53:24.623579Z","iopub.execute_input":"2025-10-23T08:53:24.623907Z","iopub.status.idle":"2025-10-23T08:58:53.449159Z","shell.execute_reply.started":"2025-10-23T08:53:24.623882Z","shell.execute_reply":"2025-10-23T08:58:53.447181Z"}},"outputs":[{"name":"stdout","text":"Output directory created/exists at: /kaggle/working/data/preprocessed\nRunning Preprocessing Script...\nLoading data...\nLoaded Train shape: (574945, 341)\n------------------------------\nStarting Full Preprocessing Pipeline...\nImputing with TOF fill value: 500...\nImputation complete.\nCorrecting handedness...\nHandedness correction complete.\nCorrecting upside-down subjects...\nUpside-down correction complete.\nCalculating linear acceleration...\nLinear acceleration added.\nAdding basic features (magnitudes)...\nBasic features added.\nFitting StandardScaler on 338 features.\nScaler fitted.\nFull Preprocessing Pipeline Finished.\n------------------------------\nProcessed Train shape: (574945, 347)\nNumber of features scaled: 338\nBranch Features: IMU=13, THM=5, TOF=320\n\nSaving preprocessor objects...\nObjects saved to: /kaggle/working/data/preprocessed\nEncoding target and saving encoder...\nTarget Classes (18): ['Above ear - pull hair' 'Cheek - pinch skin' 'Drink from bottle/cup'\n 'Eyebrow - pull hair' 'Eyelash - pull hair'\n 'Feel around in tray and pull out an object' 'Forehead - pull hairline'\n 'Forehead - scratch' 'Glasses on/off' 'Neck - pinch skin'\n 'Neck - scratch' 'Pinch knee/leg skin' 'Pull air toward your face'\n 'Scratch knee/leg skin' 'Text on phone' 'Wave hello' 'Write name in air'\n 'Write name on leg']\nLabel encoder saved to: /kaggle/working/data/preprocessed/label_encoder.joblib\n\nSaving processed training data to /kaggle/working/data/preprocessed/train_processed.parquet...\nProcessed data saved.\n\nPreprocessing script finished.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nimport os\nfrom tqdm import tqdm # For progress bar\n\n# ==============================================================================\n#                            CONFIGURATION\n# ==============================================================================\n# --- Input Paths ---\n# Should match the OUTPUT_DIR and filenames from preprocessing.py\nPREPROCESSED_DIR = './data/preprocessed'\nPROCESSED_TRAIN_PATH = os.path.join(PREPROCESSED_DIR, 'train_processed.parquet')\nALL_COLS_PATH = os.path.join(PREPROCESSED_DIR, 'all_feature_cols_3branch.pkl') # List of scaled + passthrough\n\n# --- Model Input Parameters ---\nMAX_LENGTH = 192 # Sequence length for padding/truncating\n\n# --- Output Paths (Optional: Save the prepared arrays) ---\nOUTPUT_NP_DIR = './data/model_input'\nX_PATH = os.path.join(OUTPUT_NP_DIR, 'X_train.npy')\nY_PATH = os.path.join(OUTPUT_NP_DIR, 'y_train.npy')\nGROUPS_PATH = os.path.join(OUTPUT_NP_DIR, 'groups_train.npy')\n\nos.makedirs(OUTPUT_NP_DIR, exist_ok=True)\n\n\n# ==============================================================================\n#                      LOAD PREPROCESSED DATA & OBJECTS\n# ==============================================================================\nprint(\"Loading preprocessed data and feature lists...\")\ntry:\n    train_processed_df = pd.read_parquet(PROCESSED_TRAIN_PATH)\n    # Load the list of all columns that were included after preprocessing (scaled + passthrough)\n    final_column_names = joblib.load(ALL_COLS_PATH)\n    # Identify which of these are the features (i.e., not IDs, labels, etc.)\n    # Define columns that are NOT features\n    non_feature_cols = [\n        'sequence_id', 'subject', 'gesture', 'row_id', 'sequence_counter',\n        'orientation', 'behavior', 'phase', 'sequence_type', 'handedness', # If kept\n        'gesture_encoded' # The target label\n    ]\n    # Filter final_column_names to get only feature columns\n    feature_cols = sorted([col for col in final_column_names if col not in non_feature_cols])\n\n    print(f\"Loaded processed data with shape: {train_processed_df.shape}\")\n    print(f\"Identified {len(feature_cols)} feature columns.\")\n    # print(\"Feature columns:\", feature_cols) # Uncomment to verify features\n\n    # Verify essential columns exist\n    if 'sequence_id' not in train_processed_df.columns: raise ValueError(\"Missing 'sequence_id'\")\n    if 'gesture_encoded' not in train_processed_df.columns: raise ValueError(\"Missing 'gesture_encoded'\")\n    if 'subject' not in train_processed_df.columns: raise ValueError(\"Missing 'subject'\")\n\nexcept FileNotFoundError as e:\n    print(f\"Error: Required file not found. Ensure preprocessing script ran successfully. Missing: {e.filename}\")\n    exit()\nexcept Exception as e:\n    print(f\"An error occurred during loading: {e}\")\n    exit()\n\n# ==============================================================================\n#                      PREPARE MODEL INPUT ARRAYS\n# ==============================================================================\nprint(\"\\nPreparing model input arrays (Grouping, Padding/Truncating)...\")\n\nall_sequences = []\nall_labels = []\nall_groups = []\nnum_features = len(feature_cols)\n\n# Group by sequence ID\ngrouped_data = train_processed_df.groupby('sequence_id')\ntotal_sequences = len(grouped_data)\nprint(f\"Processing {total_sequences} sequences...\")\n\n# Iterate through each sequence group\nfor name, group in tqdm(grouped_data, total=total_sequences, desc=\"Processing Sequences\"):\n    # Extract features, label, and group\n    sequence_features = group[feature_cols].values.astype(np.float32) # Ensure float32\n    label = group['gesture_encoded'].iloc[0] # Get the single label for the sequence\n    subject = group['subject'].iloc[0]       # Get the subject ID\n\n    current_length = sequence_features.shape[0]\n    padded_sequence = np.zeros((MAX_LENGTH, num_features), dtype=np.float32) # Initialize with zeros\n\n    if current_length == 0:\n        print(f\"Warning: Sequence {name} is empty. Skipping.\")\n        continue # Skip empty sequences\n\n    # Pad or Truncate\n    if current_length >= MAX_LENGTH:\n        # Truncate: Take the last MAX_LENGTH steps\n        padded_sequence = sequence_features[-MAX_LENGTH:]\n    else:\n        # Pad: Add zeros at the beginning (pre-padding)\n        pad_width = MAX_LENGTH - current_length\n        padded_sequence[pad_width:] = sequence_features\n\n    # Append results to lists\n    all_sequences.append(padded_sequence)\n    all_labels.append(label)\n    all_groups.append(subject)\n\n# Convert lists to NumPy arrays\nX = np.array(all_sequences)\ny = np.array(all_labels)\ngroups = np.array(all_groups)\n\nprint(\"\\nModel input preparation complete.\")\nprint(f\"Shape of X: {X.shape}\") # Should be (num_sequences, MAX_LENGTH, num_features)\nprint(f\"Shape of y: {y.shape}\")   # Should be (num_sequences,)\nprint(f\"Shape of groups: {groups.shape}\") # Should be (num_sequences,)\n\n# ==============================================================================\n#                      SAVE MODEL INPUT ARRAYS (Optional)\n# ==============================================================================\nprint(f\"\\nSaving prepared NumPy arrays to {OUTPUT_NP_DIR}...\")\ntry:\n    np.save(X_PATH, X)\n    np.save(Y_PATH, y)\n    np.save(GROUPS_PATH, groups)\n    print(\"Arrays saved successfully.\")\nexcept Exception as e:\n    print(f\"Error saving NumPy arrays: {e}\")\n\nprint(\"\\nScript finished. You now have X, y, and groups ready for model training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T08:58:53.451694Z","iopub.execute_input":"2025-10-23T08:58:53.452134Z","iopub.status.idle":"2025-10-23T08:59:05.986189Z","shell.execute_reply.started":"2025-10-23T08:58:53.452089Z","shell.execute_reply":"2025-10-23T08:59:05.985186Z"}},"outputs":[{"name":"stdout","text":"Loading preprocessed data and feature lists...\nLoaded processed data with shape: (574945, 348)\nIdentified 338 feature columns.\n\nPreparing model input arrays (Grouping, Padding/Truncating)...\nProcessing 8151 sequences...\n","output_type":"stream"},{"name":"stderr","text":"Processing Sequences: 100%|██████████| 8151/8151 [00:07<00:00, 1040.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nModel input preparation complete.\nShape of X: (8151, 192, 338)\nShape of y: (8151,)\nShape of groups: (8151,)\n\nSaving prepared NumPy arrays to ./data/model_input...\nArrays saved successfully.\n\nScript finished. You now have X, y, and groups ready for model training.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Model Building and Evaluation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.optimize import linear_sum_assignment\nimport lightgbm as lgb\nimport joblib\nimport gc\nimport os\nimport warnings\nfrom tqdm import tqdm # Keep tqdm if installed, otherwise remove\n\nwarnings.filterwarnings('ignore')\n\nprint(\"Starting LGBM Pipeline...\")\n# ==============================================================================\n#                            CONFIGURATION\n# ==============================================================================\n# --- Input Paths ---\nPREPROCESSED_DIR = './data/preprocessed'\nPROCESSED_TRAIN_PATH = os.path.join(PREPROCESSED_DIR, 'train_processed.parquet')\nLABEL_ENCODER_PATH = os.path.join(PREPROCESSED_DIR, 'label_encoder.joblib')\nALL_COLS_PATH = os.path.join(PREPROCESSED_DIR, 'all_feature_cols_3branch.pkl') # List of all scaled columns from preprocessor\nIMU_COLS_PATH = os.path.join(PREPROCESSED_DIR, 'imu_feature_cols_3branch.pkl') # List of IMU-related scaled columns from preprocessor\n\n# --- Model/Output Paths ---\nOUTPUT_DIR = './data/lgbm_simple_output' # New directory\nMODEL_ALL_TMPL = os.path.join(OUTPUT_DIR, \"model_lgbm_simple_all_fold_{fold}.txt\")\nMODEL_IMU_TMPL = os.path.join(OUTPUT_DIR, \"model_lgbm_simple_imu_fold_{fold}.txt\")\nOOF_CSV_PATH = os.path.join(OUTPUT_DIR, \"oof_predictions_lgbm_simple.csv\")\nLGBM_ALL_FEATURES_PATH = os.path.join(OUTPUT_DIR, 'lgbm_simple_all_feature_names.pkl')\nLGBM_IMU_FEATURES_PATH = os.path.join(OUTPUT_DIR, 'lgbm_simple_imu_feature_names.pkl')\n\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# --- Training Parameters ---\nN_SPLITS = 2 # Changed back to 5 splits\nSEED = 42\n\n# LGBM Parameters (Simplified, tune these)\nLGBM_PARAMS = {\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'n_estimators': 1500, # Use early stopping\n    'learning_rate': 0.03,\n    'feature_fraction': 0.7, # Reduced slightly\n    'bagging_fraction': 0.7, # Reduced slightly\n    'bagging_freq': 1,\n    'lambda_l1': 0.2,\n    'lambda_l2': 0.2,\n    'num_leaves': 31, # Kept moderate\n    'verbose': -1,\n    'n_jobs': -1,\n    'seed': SEED,\n    'boosting_type': 'gbdt',\n    # 'num_class': 18 # Will be set dynamically\n}\n\n# --- Target Gestures ---\nTARGET_GESTURES = [\n    'Above ear - pull hair', 'Cheek - pinch skin', 'Eyebrow - pull hair',\n    'Eyelash - pull hair', 'Forehead - pull hairline', 'Forehead - scratch',\n    'Neck - pinch skin', 'Neck - scratch'\n]\n\nnp.random.seed(SEED)\n\n# ==============================================================================\n#                      LOAD PREPROCESSED DATA & OBJECTS\n# ==============================================================================\nprint(\"Loading preprocessed data and objects...\")\ntry:\n    train_processed_df = pd.read_parquet(PROCESSED_TRAIN_PATH)\n    label_encoder = joblib.load(LABEL_ENCODER_PATH)\n    all_feature_cols = joblib.load(ALL_COLS_PATH) # All scaled + engineered features from preprocessor\n    imu_feature_cols = joblib.load(IMU_COLS_PATH) # Only IMU related scaled + engineered from preprocessor\n\n    # --- Verification ---\n    required_original_cols = ['sequence_id', 'gesture_encoded', 'subject']\n    if not all(col in train_processed_df.columns for col in required_original_cols):\n        raise ValueError(f\"Missing one or more required columns: {required_original_cols}\")\n    # Ensure feature lists are subsets of dataframe columns\n    all_feature_cols = [c for c in all_feature_cols if c in train_processed_df.columns]\n    imu_feature_cols = [c for c in imu_feature_cols if c in train_processed_df.columns]\n    if not all_feature_cols: raise ValueError(\"No ALL feature columns found in DataFrame after loading list.\")\n    if not imu_feature_cols: raise ValueError(\"No IMU feature columns found in DataFrame after loading list.\")\n    # --- End Verification ---\n\n    N_CLASSES = len(label_encoder.classes_)\n    LGBM_PARAMS['num_class'] = N_CLASSES\n\n    print(f\"Loaded processed data with shape: {train_processed_df.shape}\")\n    print(f\"Number of ALL base features identified: {len(all_feature_cols)}\")\n    print(f\"Number of IMU base features identified: {len(imu_feature_cols)}\")\n    print(f\"Number of classes: {N_CLASSES}\")\n\nexcept FileNotFoundError as e:\n    print(f\"Error: Required file not found. Ensure preprocessing ran. Missing: {e.filename}\"); exit()\nexcept Exception as e:\n    print(f\"An error occurred during loading: {e}\"); exit()\n\n# ==============================================================================\n#            FEATURE AGGREGATION FOR LGBM (Reduced Statistics)\n# ==============================================================================\n\ndef aggregate_features_for_lgbm_simple(df, feature_cols_to_agg, sequence_id_col='sequence_id', subject_col='subject', label_col='gesture_encoded'):\n    \"\"\" Aggregates time-series features using a reduced set of statistics. \"\"\"\n    print(f\"Aggregating {len(feature_cols_to_agg)} features (Simple Stats)...\")\n\n    # --- REDUCED Aggregations ---\n    # Core stats + maybe first/last\n    aggs = ['mean', 'std', 'min', 'max', 'median', 'first', 'last']\n    # If computation time still too long, reduce further to ['mean', 'std', 'min', 'max']\n\n    # Group by sequence and aggregate\n    agg_dict = {col: aggs for col in feature_cols_to_agg}\n\n    # Use tqdm if available\n    try:\n        from tqdm.auto import tqdm\n        tqdm.pandas()\n        agg_df = df.groupby(sequence_id_col).progress_agg(agg_dict) # Requires tqdm>=4.42.0\n    except ImportError:\n        print(\"tqdm not found or version too old, aggregation progress bar disabled.\")\n        agg_df = df.groupby(sequence_id_col).agg(agg_dict)\n    except AttributeError: # If progress_agg doesn't exist\n         print(\"progress_agg not available, using standard agg.\")\n         agg_df = df.groupby(sequence_id_col).agg(agg_dict)\n\n\n    # Flatten multi-level column index\n    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]\n    agg_df = agg_df.reset_index()\n\n    # Add sequence-level metadata\n    meta_cols = [sequence_id_col, subject_col, label_col]\n    meta_df = df[meta_cols].drop_duplicates(subset=[sequence_id_col])\n\n    # Merge aggregated features with metadata\n    final_df = pd.merge(meta_df, agg_df, on=sequence_id_col, how='left')\n\n    # Fill NaNs/Infs\n    final_df = final_df.fillna(0)\n    final_df.replace([np.inf, -np.inf], 0, inplace=True)\n\n    print(f\"Aggregation complete. Shape: {final_df.shape}\")\n    return final_df\n\n# --- Aggregate for ALL features ---\ntrain_agg_all_df = aggregate_features_for_lgbm_simple(train_processed_df, all_feature_cols)\nlgbm_all_feature_names = [col for col in train_agg_all_df.columns if col not in ['sequence_id', 'subject', 'gesture_encoded']]\nprint(f\"Number of aggregated features (ALL): {len(lgbm_all_feature_names)}\")\n\n# --- Aggregate for IMU features ---\ntrain_agg_imu_df = aggregate_features_for_lgbm_simple(train_processed_df, imu_feature_cols)\nlgbm_imu_feature_names = [col for col in train_agg_imu_df.columns if col not in ['sequence_id', 'subject', 'gesture_encoded']]\nprint(f\"Number of aggregated features (IMU): {len(lgbm_imu_feature_names)}\")\n\n# Save the list of aggregated feature names\njoblib.dump(lgbm_all_feature_names, LGBM_ALL_FEATURES_PATH)\njoblib.dump(lgbm_imu_feature_names, LGBM_IMU_FEATURES_PATH)\nprint(\"Saved aggregated feature name lists.\")\n\ndel train_processed_df; gc.collect() # Clean up\n\n# ==============================================================================\n#                      COMPETITION METRIC FUNCTION (UNCHANGED)\n# ==============================================================================\ndef hierarchical_macro_f1(y_true, y_pred_labels, target_gestures_list, le):\n    \"\"\"Calculates the CMI competition metric with corrected variable scope.\"\"\"\n    # Ensure inputs are not empty first\n    if len(y_true) == 0 or len(y_pred_labels) == 0:\n        print(\"Warning: hierarchical_macro_f1 received empty input.\")\n        return 0.0, 0.0, 0.0\n\n    # --- Ensure assignments happen AFTER the initial check ---\n    y_true_np = np.asarray(y_true)\n    y_pred_labels_np = np.asarray(y_pred_labels)\n    # --- End Assignment Fix ---\n\n    # --- Check for valid label indices ---\n    known_labels = np.arange(len(le.classes_))\n    # Create mask where predicted labels are within the known range\n    valid_pred_mask = np.isin(y_pred_labels_np, known_labels)\n\n    # Filter both true and predicted labels based on the mask\n    y_true_filtered = y_true_np[valid_pred_mask]\n    y_pred_filtered = y_pred_labels_np[valid_pred_mask]\n\n    # If filtering removed all samples, return 0\n    if len(y_true_filtered) == 0:\n        print(\"Warning: No valid predictions found after filtering in hierarchical_macro_f1.\")\n        return 0.0, 0.0, 0.0\n    # --- End Validity Check ---\n\n    # --- Proceed with metric calculation using filtered arrays ---\n    try:\n        y_true_str = le.inverse_transform(y_true_filtered)\n        y_pred_str = le.inverse_transform(y_pred_filtered)\n    except ValueError as e:\n        print(f\"LabelEncoder Error in metric: {e}. Check label range. True: {np.unique(y_true_filtered)}, Pred: {np.unique(y_pred_filtered)}\")\n        return 0.0, 0.0, 0.0 # Return 0 if inverse_transform fails\n\n    y_true_bin = np.isin(y_true_str, target_gestures_list)\n    y_pred_bin = np.isin(y_pred_str, target_gestures_list)\n    binary_f1 = f1_score(y_true_bin, y_pred_bin, pos_label=True, zero_division=0)\n\n    y_true_mc = np.where(y_true_bin, y_true_str, 'non_target')\n    y_pred_mc = np.where(y_pred_bin, y_pred_str, 'non_target')\n    unique_labels_mc_present = np.unique(np.concatenate((y_true_mc, y_pred_mc)))\n\n    # Ensure 'non_target' class is handled if present\n    # Get all possible labels expected by f1_score\n    all_possible_classes = np.append(le.classes_, 'non_target')\n    labels_for_f1 = [lbl for lbl in all_possible_classes if lbl in unique_labels_mc_present]\n\n\n    gesture_f1 = f1_score(y_true_mc, y_pred_mc, labels=labels_for_f1, average='macro', zero_division=0)\n\n    metric = 0.5 * binary_f1 + 0.5 * gesture_f1\n    return metric, binary_f1, gesture_f1\n# ==============================================================================\n#                      CROSS-VALIDATION TRAINING (LGBM)\n# ==============================================================================\nprint(f\"\\nStarting {N_SPLITS}-Fold Cross-Validation (LGBM - Simple Features)...\")\n\n# --- Prepare data for CV ---\nX_all = train_agg_all_df[lgbm_all_feature_names]\nX_imu = train_agg_imu_df[lgbm_imu_feature_names]\ny = train_agg_all_df['gesture_encoded']\ngroups = train_agg_all_df['subject']\nsequence_ids_cv = train_agg_all_df['sequence_id']\n\n# Use pandas DataFrames directly for LGBM (often easier)\ngkf = GroupKFold(n_splits=N_SPLITS)\n\n# OOF storage\noof_preds_all = np.zeros((len(train_agg_all_df), N_CLASSES))\noof_preds_imu = np.zeros((len(train_agg_imu_df), N_CLASSES))\noof_labels_all = np.zeros(len(train_agg_all_df))\noof_labels_imu = np.zeros(len(train_agg_imu_df))\noof_groups_store = np.empty(len(train_agg_all_df), dtype=object)\noof_seq_ids_store = np.zeros(len(train_agg_all_df))\n\nfold_metrics_all = []\nfold_metrics_imu = []\nfeature_importances_all = pd.DataFrame()\nfeature_importances_imu = pd.DataFrame()\n\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X_all, y, groups)):\n    print(f\"\\n===== Fold {fold+1}/{N_SPLITS} =====\")\n\n    # --- Data for ALL Features Model ---\n    X_train_all_fold, X_val_all_fold = X_all.iloc[train_idx], X_all.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\n    # --- Data for IMU Features Model ---\n    # Need to align IMU data with the same train/val indices\n    X_imu_aligned = train_agg_imu_df.set_index('sequence_id').loc[sequence_ids_cv].reset_index() # Ensure same order as X_all\n    X_train_imu_fold, X_val_imu_fold = X_imu_aligned.iloc[train_idx][lgbm_imu_feature_names], X_imu_aligned.iloc[val_idx][lgbm_imu_feature_names]\n\n    # Store validation info\n    oof_groups_store[val_idx] = groups.iloc[val_idx].values\n    oof_seq_ids_store[val_idx] = sequence_ids_cv.iloc[val_idx].values\n\n    # --- Train ALL Features Model ---\n    print(f\"\\n--- Training ALL Features Model (Fold {fold+1}) ---\")\n    model_all = lgb.LGBMClassifier(**LGBM_PARAMS)\n    model_all.fit(X_train_all_fold, y_train_fold,\n                  eval_set=[(X_val_all_fold, y_val_fold)],\n                  callbacks=[lgb.early_stopping(150, verbose=False), lgb.log_evaluation(period=500)]) # Log less often\n\n    fold_oof_preds_all = model_all.predict_proba(X_val_all_fold)\n    oof_preds_all[val_idx] = fold_oof_preds_all\n    oof_labels_all[val_idx] = np.argmax(fold_oof_preds_all, axis=1)\n    metric_all, _, _ = hierarchical_macro_f1(y_val_fold, np.argmax(fold_oof_preds_all, axis=1), TARGET_GESTURES, label_encoder)\n    fold_metrics_all.append(metric_all)\n    print(f\"Fold {fold+1} ALL Features Hierarchical F1: {metric_all:.4f}\")\n    model_all.booster_.save_model(MODEL_ALL_TMPL.format(fold=fold+1)); print(f\"ALL features model saved.\")\n    fold_importance_all_df = pd.DataFrame({\"feature\": lgbm_all_feature_names, \"importance\": model_all.feature_importances_, \"fold\": fold + 1})\n    feature_importances_all = pd.concat([feature_importances_all, fold_importance_all_df], axis=0)\n\n    # --- Train IMU Features Model ---\n    print(f\"\\n--- Training IMU Features Model (Fold {fold+1}) ---\")\n    model_imu = lgb.LGBMClassifier(**LGBM_PARAMS)\n    model_imu.fit(X_train_imu_fold, y_train_fold, # Use same y_train_fold\n                  eval_set=[(X_val_imu_fold, y_val_fold)], # Use same y_val_fold\n                  callbacks=[lgb.early_stopping(150, verbose=False), lgb.log_evaluation(period=500)])\n\n    fold_oof_preds_imu = model_imu.predict_proba(X_val_imu_fold)\n    oof_preds_imu[val_idx] = fold_oof_preds_imu\n    oof_labels_imu[val_idx] = np.argmax(fold_oof_preds_imu, axis=1)\n    metric_imu, _, _ = hierarchical_macro_f1(y_val_fold, np.argmax(fold_oof_preds_imu, axis=1), TARGET_GESTURES, label_encoder)\n    fold_metrics_imu.append(metric_imu)\n    print(f\"Fold {fold+1} IMU Features Hierarchical F1: {metric_imu:.4f}\")\n    model_imu.booster_.save_model(MODEL_IMU_TMPL.format(fold=fold+1)); print(f\"IMU features model saved.\")\n    fold_importance_imu_df = pd.DataFrame({\"feature\": lgbm_imu_feature_names, \"importance\": model_imu.feature_importances_, \"fold\": fold + 1})\n    feature_importances_imu = pd.concat([feature_importances_imu, fold_importance_imu_df], axis=0)\n\n    del X_train_all_fold, X_val_all_fold, X_train_imu_fold, X_val_imu_fold, y_train_fold, y_val_fold\n    del model_all, model_imu; gc.collect()\n\n# ==============================================================================\n#                      OVERALL OOF EVALUATION\n# ==============================================================================\nprint(\"\\n===== Overall OOF Evaluation (LGBM Simple - Before PP) =====\")\n# --- ALL Features ---\nvalid_oof_mask_all = np.isin(oof_labels_all.astype(int), np.arange(N_CLASSES))\noof_metric_overall_all, oof_bin_f1_all, oof_gest_f1_all = hierarchical_macro_f1(y.iloc[valid_oof_mask_all].values, oof_labels_all.astype(int)[valid_oof_mask_all], TARGET_GESTURES, label_encoder)\nprint(f\"Overall OOF ALL Features Hierarchical F1: {oof_metric_overall_all:.4f}\")\nprint(f\"Mean Fold Metric (ALL): {np.mean(fold_metrics_all):.4f} +/- {np.std(fold_metrics_all):.4f}\")\nprint(\"\\nOOF Classification Report (ALL Features - Before PP):\")\nprint(classification_report(y.iloc[valid_oof_mask_all].values, oof_labels_all.astype(int)[valid_oof_mask_all], target_names=label_encoder.classes_, zero_division=0))\n\n# --- IMU Features ---\nprint(\"\\n--- IMU Features ---\")\nvalid_oof_mask_imu = np.isin(oof_labels_imu.astype(int), np.arange(N_CLASSES))\noof_metric_overall_imu, oof_bin_f1_imu, oof_gest_f1_imu = hierarchical_macro_f1(y.iloc[valid_oof_mask_imu].values, oof_labels_imu.astype(int)[valid_oof_mask_imu], TARGET_GESTURES, label_encoder)\nprint(f\"Overall OOF IMU Features Hierarchical F1: {oof_metric_overall_imu:.4f}\")\nprint(f\"Mean Fold Metric (IMU): {np.mean(fold_metrics_imu):.4f} +/- {np.std(fold_metrics_imu):.4f}\")\n\n# --- Save OOF Predictions (using ALL features primarily) ---\noof_df = pd.DataFrame({'sequence_id': oof_seq_ids_store, 'subject': oof_groups_store, 'true_label': y.values, 'pred_label_raw': oof_labels_all.astype(int)})\nfor i in range(N_CLASSES): oof_df[f'pred_proba_{i}'] = oof_preds_all[:, i]\noof_df.to_csv(OOF_CSV_PATH, index=False); print(f\"\\nOOF predictions saved to {OOF_CSV_PATH}\")\n\nprint(\"\\n===== FULL LGBM SIMPLE PIPELINE FINISHED (Training and OOF Evaluation Only) =====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:40:54.185571Z","iopub.execute_input":"2025-10-23T16:40:54.185976Z","iopub.status.idle":"2025-10-23T16:51:14.114643Z","shell.execute_reply.started":"2025-10-23T16:40:54.185952Z","shell.execute_reply":"2025-10-23T16:51:14.113530Z"}},"outputs":[{"name":"stdout","text":"Starting LGBM Pipeline...\nLoading preprocessed data and objects...\nLoaded processed data with shape: (574945, 348)\nNumber of ALL base features identified: 338\nNumber of IMU base features identified: 13\nNumber of classes: 18\nAggregating 338 features (Simple Stats)...\nprogress_agg not available, using standard agg.\nAggregation complete. Shape: (8151, 2369)\nNumber of aggregated features (ALL): 2366\nAggregating 13 features (Simple Stats)...\nprogress_agg not available, using standard agg.\nAggregation complete. Shape: (8151, 94)\nNumber of aggregated features (IMU): 91\nSaved aggregated feature name lists.\n\nStarting 2-Fold Cross-Validation (LGBM - Simple Features)...\n\n===== Fold 1/2 =====\n\n--- Training ALL Features Model (Fold 1) ---\nFold 1 ALL Features Hierarchical F1: 0.7505\nALL features model saved.\n\n--- Training IMU Features Model (Fold 1) ---\nFold 1 IMU Features Hierarchical F1: 0.6619\nIMU features model saved.\n\n===== Fold 2/2 =====\n\n--- Training ALL Features Model (Fold 2) ---\nFold 2 ALL Features Hierarchical F1: 0.7679\nALL features model saved.\n\n--- Training IMU Features Model (Fold 2) ---\nFold 2 IMU Features Hierarchical F1: 0.6566\nIMU features model saved.\n\n===== Overall OOF Evaluation (LGBM Simple - Before PP) =====\nOverall OOF ALL Features Hierarchical F1: 0.7592\nMean Fold Metric (ALL): 0.7592 +/- 0.0087\n\nOOF Classification Report (ALL Features - Before PP):\n                                            precision    recall  f1-score   support\n\n                     Above ear - pull hair       0.67      0.71      0.69       638\n                        Cheek - pinch skin       0.49      0.51      0.50       637\n                     Drink from bottle/cup       0.85      0.75      0.80       161\n                       Eyebrow - pull hair       0.43      0.36      0.39       638\n                       Eyelash - pull hair       0.45      0.44      0.45       640\nFeel around in tray and pull out an object       0.95      0.86      0.91       161\n                  Forehead - pull hairline       0.62      0.58      0.60       640\n                        Forehead - scratch       0.59      0.63      0.61       640\n                            Glasses on/off       0.88      0.74      0.80       161\n                         Neck - pinch skin       0.43      0.42      0.43       640\n                            Neck - scratch       0.44      0.51      0.47       640\n                       Pinch knee/leg skin       0.36      0.27      0.31       161\n                 Pull air toward your face       0.64      0.64      0.64       477\n                     Scratch knee/leg skin       0.39      0.31      0.34       161\n                             Text on phone       0.79      0.91      0.84       640\n                                Wave hello       0.70      0.71      0.70       478\n                         Write name in air       0.64      0.63      0.64       477\n                         Write name on leg       0.39      0.42      0.40       161\n\n                                  accuracy                           0.58      8151\n                                 macro avg       0.60      0.58      0.59      8151\n                              weighted avg       0.58      0.58      0.58      8151\n\n\n--- IMU Features ---\nOverall OOF IMU Features Hierarchical F1: 0.6595\nMean Fold Metric (IMU): 0.6593 +/- 0.0026\n\nOOF predictions saved to ./data/lgbm_simple_output/oof_predictions_lgbm_simple.csv\n\n===== FULL LGBM SIMPLE PIPELINE FINISHED (Training and OOF Evaluation Only) =====\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}